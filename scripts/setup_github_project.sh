#!/usr/bin/env bash
#
# setup_github_project.sh — Create GitHub labels, milestones, and issues for VMAT Diffusion
#
# Prerequisites:
#   1. gh CLI installed: https://cli.github.com/
#   2. Authenticated: gh auth login
#   3. Run from the repo root: bash scripts/setup_github_project.sh
#
# This script is idempotent — safe to re-run (labels skip if they exist,
# issues check for duplicates by title before creating).
#
# Attribution: Generated by Claude for the VMAT Diffusion project, 2026-02-21.

set -euo pipefail

REPO="wrockey/vmat-diffusion"

echo "============================================="
echo "VMAT Diffusion — GitHub Project Setup"
echo "Repo: $REPO"
echo "============================================="

# -------------------------------------------------------------------
# 1. LABELS
# -------------------------------------------------------------------
echo ""
echo ">>> Creating labels..."

create_label() {
    local name="$1" color="$2" description="$3"
    if gh label list --repo "$REPO" --json name --jq '.[].name' | grep -qx "$name"; then
        echo "  [skip] $name (exists)"
    else
        gh label create "$name" --repo "$REPO" --color "$color" --description "$description"
        echo "  [created] $name"
    fi
}

# Phase labels (blue gradient)
create_label "phase/0-setup"     "0052CC" "Phase 0: Work machine setup, data collection, pipeline fixes"
create_label "phase/1-eval"      "1D76DB" "Phase 1: Clinical evaluation framework"
create_label "phase/2-combined"  "5B9BD5" "Phase 2: Combined loss experiment"
create_label "phase/3-iterate"   "9FC5E8" "Phase 3: Iteration, publication prep"

# Type labels
create_label "type/experiment"   "0E8A16" "A specific experiment to design, run, or analyze"
create_label "type/decision"     "B60205" "Decision branch point with rationale"
create_label "type/backburner"   "8B8B8B" "Revisit later — not blocking current work"
create_label "type/if-stuck"     "FBCA04" "Alternative approach to try if current path plateaus"
create_label "type/pipeline"     "D4C5F9" "Data preprocessing or evaluation pipeline work"
create_label "type/publication"  "F9D0C4" "Publication-related task (writing, figures, submission)"

# Priority labels
create_label "priority/critical" "B60205" "Blocking progress — must resolve before continuing"
create_label "priority/high"     "D93F0B" "Important but not immediately blocking"
create_label "priority/low"      "C2E0C6" "Nice to have, do when convenient"

# Status labels (supplement GitHub's open/closed)
create_label "status/blocked"    "E4E669" "Waiting on external dependency or prerequisite"
create_label "status/needs-data" "BFDADC" "Cannot proceed until dataset is available"

# Delete some noisy default labels
for label in "good first issue" "help wanted" "invalid" "duplicate" "wontfix" "question"; do
    if gh label list --repo "$REPO" --json name --jq '.[].name' | grep -qx "$label"; then
        gh label delete "$label" --repo "$REPO" --yes 2>/dev/null || true
        echo "  [deleted] $label (default, unused)"
    fi
done

# -------------------------------------------------------------------
# 2. MILESTONES
# -------------------------------------------------------------------
echo ""
echo ">>> Creating milestones..."

create_milestone() {
    local title="$1" description="$2"
    if gh api "repos/$REPO/milestones" --jq '.[].title' | grep -qx "$title"; then
        echo "  [skip] $title (exists)"
    else
        gh api "repos/$REPO/milestones" -f title="$title" -f description="$description" -f state="open" > /dev/null
        echo "  [created] $title"
    fi
}

create_milestone "Phase 0: Setup" "Work machine setup, data collection (100+ cases), D95 pipeline fix, environment verification"
create_milestone "Phase 1: Evaluation Framework" "Build clinical evaluation framework before training — defines what 'good' means"
create_milestone "Phase 2: Combined Loss" "First real experiment: combined 5-component loss with uncertainty weighting on 100+ cases"
create_milestone "Phase 3: Iterate & Publish" "Result-driven iteration, publication preparation, code release"

# Milestone titles (gh issue create --milestone expects the title string, not number)
MS0="Phase 0: Setup"
MS1="Phase 1: Evaluation Framework"
MS2="Phase 2: Combined Loss"
MS3="Phase 3: Iterate & Publish"

# -------------------------------------------------------------------
# 3. ISSUES
# -------------------------------------------------------------------
echo ""
echo ">>> Creating issues..."

create_issue() {
    local title="$1" body="$2" labels="$3" milestone="$4"
    # Check if issue with this title already exists
    if gh issue list --repo "$REPO" --state all --json title --jq '.[].title' | grep -qxF "$title"; then
        echo "  [skip] $title (exists)"
    else
        local cmd="gh issue create --repo $REPO --title"
        if [ -n "$milestone" ]; then
            gh issue create --repo "$REPO" --title "$title" --body "$body" --label "$labels" --milestone "$milestone"
        else
            gh issue create --repo "$REPO" --title "$title" --body "$body" --label "$labels"
        fi
        echo "  [created] $title"
    fi
}

# ---- PHASE 0 ISSUES ----

create_issue \
    "Set up WSL development environment on work machine" \
    "$(cat <<'BODY'
## Task
Clone repo, install conda environment, verify GPU access.

## Steps
1. Install WSL2 + Ubuntu on work machine
2. Install CUDA toolkit + drivers (verify with `nvidia-smi`)
3. Clone repo: `git clone https://github.com/wrockey/vmat-diffusion.git`
4. Create conda env: `conda env create -f environment.yml && conda activate vmat-diffusion`
5. Verify GPU: `python -c "import torch; print(torch.cuda.get_device_name(0))"`
6. Store data on Linux filesystem (`/home/...`), NOT on Windows mounts (`/mnt/c/...`)
7. Update `.claude/instructions.md` PLATFORM REFERENCE section with actual paths

## Acceptance criteria
- [ ] `nvidia-smi` shows RTX 3090
- [ ] `torch.cuda.is_available()` returns True
- [ ] Conda env activates without errors
- [ ] Platform reference updated in docs
BODY
)" \
    "phase/0-setup" \
    "$MS0"

create_issue \
    "Collect and anonymize 100+ DICOM-RT cases" \
    "$(cat <<'BODY'
## Task
Gather and de-identify 100+ prostate VMAT DICOM-RT datasets for training.

## Requirements
- Minimum 100 cases (target 150) of prostate cancer with SIB protocol
- 70 Gy PTV70 (prostate) / 56 Gy PTV56 (seminal vesicles) in 28 fractions
- Each case needs: CT, RT Structure Set, RT Dose, RT Plan
- All 8 required structures present: PTV70, PTV56, Prostate, Rectum, Bladder, Femur_L, Femur_R, Bowel

## De-identification
- DICOM de-identification (remove PHI from all tags)
- Document anonymization method for publication
- Maintain case mapping (internal ID → NPZ case ID)
- Record any excluded cases with reasons

## Acceptance criteria
- [ ] 100+ de-identified DICOM-RT datasets collected
- [ ] Each case has all required DICOM objects
- [ ] Anonymization method documented
- [ ] Case mapping recorded
- [ ] Exclusion criteria and excluded cases documented
BODY
)" \
    "phase/0-setup,status/needs-data" \
    "$MS0"

create_issue \
    "Preprocess all cases with preprocess_dicom_rt_v2.2.py" \
    "$(cat <<'BODY'
## Task
Run the preprocessing pipeline on all collected DICOM-RT cases.

## Command
```bash
python scripts/preprocess_dicom_rt_v2.2.py --skip_plots
```

## Post-processing verification
- Spot-check 3-5 cases with `notebooks/verify_npz.ipynb`
- Verify NPZ v2.2.0 format: ct, dose, masks, masks_sdf, constraints, metadata
- Check dose normalization (should be normalized to Rx)
- Verify all 8 structure channels are populated
- Check for any failed/partial cases

## Depends on
- #2 (100+ cases collected)

## Acceptance criteria
- [ ] All cases preprocessed to NPZ format
- [ ] Spot-check passes on 3-5 cases
- [ ] No failed cases (or failures documented with reasons)
- [ ] Dataset statistics documented (volume sizes, spacing, etc.)
BODY
)" \
    "phase/0-setup,type/pipeline,status/blocked" \
    "$MS0"

create_issue \
    "Fix D95 pipeline artifact — GT reads 55 Gy instead of ≥66.5 Gy" \
    "$(cat <<'BODY'
## Bug
Ground truth PTV70 D95 reads ~55 Gy when clinical plans guarantee ≥66.5 Gy (95% of 70 Gy Rx). This is a **pipeline artifact**, not a clinical finding — all delivered plans pass D95 constraints.

## Root cause
PTV mask/dose grid boundary mismatch:
- Linear dose interpolation smooths the steep dose falloff at PTV edge
- PTV mask may extend 1-2 voxels beyond TPS boundary into the falloff zone
- ~5% of PTV voxels land in the falloff region → D95 drops artificially to 55 Gy

## Investigation plan
1. Erode PTV mask by 1-2mm and recompute D95 — does it jump to ≥66.5 Gy?
2. Check dose grid coverage vs PTV spatial extent
3. Compare binary mask boundary vs SDF-derived mask boundary
4. Visualize PTV mask overlay on dose distribution at D95 slice

## Impact
**Critical:** Invalidates all DVH-based evaluation until fixed. The DVH-aware loss component trains against incorrect D95 ground truth.

## Success criteria
- [ ] After fix, GT D95 on ALL training cases reads ≥66.5 Gy
- [ ] Fix does not break structure-weighted loss computation
- [ ] Fix does not break DVH loss computation
- [ ] Root cause documented with visualization
BODY
)" \
    "phase/0-setup,bug,priority/critical" \
    "$MS0"

create_issue \
    "Document data provenance and ethics for Medical Physics submission" \
    "$(cat <<'BODY'
## Task
Prepare data provenance and ethics documentation required for Medical Physics journal submission.

## Required documentation
- [ ] IRB approval status and protocol number
- [ ] Anonymization method (DICOM de-identification procedure used)
- [ ] Informed consent / waiver documentation
- [ ] Case mapping: which raw DICOMs map to which NPZ case IDs
- [ ] Data availability statement (will data be shared? TCIA deposit?)
- [ ] Exclusion criteria and any cases excluded with reasons
- [ ] Data use agreement / institutional requirements

## Notes
This is a checklist — items may be completed incrementally as data collection progresses.
Some items (IRB, consent) may need institutional coordination.
BODY
)" \
    "phase/0-setup,type/publication" \
    "$MS0"

# ---- PHASE 1 ISSUES ----

create_issue \
    "Implement per-structure DVH compliance evaluation" \
    "$(cat <<'BODY'
## Task
Build automated per-structure DVH compliance checking against QUANTEC constraints.

## Requirements
- Compute DVH for each of 8 structures from predicted dose
- Check pass/fail against QUANTEC limits per OAR:
  - Rectum: V70 < X%, V60 < Y%, etc.
  - Bladder: V70 < X%, V65 < Y%, etc.
  - Femurs: Dmean constraints
  - Bowel: volume constraints
- PTV targets: D95 ≥ 66.5 Gy (PTV70), D95 ≥ 53.2 Gy (PTV56)
- Output: per-case compliance report (pass/fail per constraint)

## Depends on
- D95 artifact fix (#4)

## Acceptance criteria
- [ ] Per-structure DVH computed correctly for all structures
- [ ] QUANTEC compliance checked with pass/fail per constraint
- [ ] Report format finalized (JSON + human-readable)
- [ ] Validated on ground truth data (all GT plans should pass)
BODY
)" \
    "phase/1-eval,type/pipeline" \
    "$MS1"

create_issue \
    "Implement PTV-region Gamma evaluation at multiple thresholds" \
    "$(cat <<'BODY'
## Task
Implement PTV-region Gamma (3%/3mm) evaluation as the primary clinical accuracy metric.

## Definition
- Evaluate on all voxels within the **union** of PTV70 and PTV56 binary masks
- Report per-structure (PTV70 only, PTV56 only) and combined
- Compute at multiple dose thresholds: no threshold, >5%, >10%, >20%
- Report all thresholds; justify primary choice in publication

## Implementation
- Use pymedphys gamma analysis
- `--gamma_subsample 4` for speed during development
- Full resolution for final evaluation

## Depends on
- D95 artifact fix (#4) — ensures masks are spatially correct

## Acceptance criteria
- [ ] PTV-region Gamma computed at 4 dose thresholds
- [ ] Per-structure and combined reporting
- [ ] Results match manual spot-check on 2-3 cases
- [ ] Performance acceptable (~minutes per case, not hours)
BODY
)" \
    "phase/1-eval,type/pipeline" \
    "$MS1"

create_issue \
    "Build dose gradient/falloff analysis" \
    "$(cat <<'BODY'
## Task
Implement dose gradient realism metrics: monotonicity and penumbra width.

## Metrics
- **Monotonicity:** dose should decrease monotonically from PTV boundary outward
- **Penumbra width:** measure 80%-20% dose falloff distance (~6mm expected for clinical plans)
- **Gradient magnitude:** compare predicted vs GT gradient profiles at PTV boundaries

## Rationale
Physical realism of dose gradients is a proxy for plan deliverability. A prediction with correct DVH but unphysical gradients would fail clinical review.

## Acceptance criteria
- [ ] Monotonicity metric computed per-case
- [ ] Penumbra width measured and compared to GT
- [ ] Gradient profiles visualized at PTV boundaries
BODY
)" \
    "phase/1-eval,type/pipeline" \
    "$MS1"

create_issue \
    "Design single-case clinical acceptability report" \
    "$(cat <<'BODY'
## Task
Synthesize all metrics (DVH compliance, PTV-region Gamma, gradient realism, MAE) into a single go/no-go clinical acceptability assessment per case.

## Report should include
- DVH compliance: pass/fail per structure (QUANTEC)
- PTV D95 compliance: pass/fail
- PTV-region Gamma pass rate (3%/3mm)
- Gradient realism score
- Overall verdict: Acceptable / Marginal / Unacceptable
- Key failure modes identified (if any)

## Format
- JSON for programmatic analysis
- Human-readable summary (suitable for publication tables)

## Depends on
- DVH compliance (#6), PTV-region Gamma (#7), Gradient analysis (#8)

## Acceptance criteria
- [ ] Report generated for each test case
- [ ] Format supports both automated analysis and publication tables
- [ ] Validated against clinical physicist judgment on 3-5 cases
BODY
)" \
    "phase/1-eval" \
    "$MS1"

create_issue \
    "Plan physician preference ranking study" \
    "$(cat <<'BODY'
## Task
Design a blind side-by-side comparison study: predicted dose vs ground truth vs alternative plans.

## Design considerations
- Blind randomized presentation (clinician doesn't know which is predicted vs GT)
- Score on Likert scale or forced-choice preference
- Minimum 3 physicians for inter-rater reliability
- 10-15 cases minimum (full test set)
- Record: overall preference, acceptability rating, specific critique per case

## Rationale
Captures clinical quality beyond automated metrics. Gamma and DVH alone can be misleading. Strengthens Medical Physics submission significantly.

## Notes
This may require IRB amendment or separate protocol. Plan early, execute after Phase 2 results are available.

## Acceptance criteria
- [ ] Study protocol designed (blinding, scoring, sample size)
- [ ] IRB/ethics requirements identified
- [ ] 3+ physicians recruited
- [ ] Data collection instrument (survey/form) prepared
BODY
)" \
    "phase/1-eval,type/publication,priority/low" \
    "$MS1"

# ---- PHASE 2 ISSUES ----

create_issue \
    "Replace stubs in calibrate_loss_normalization.py with real loss functions" \
    "$(cat <<'BODY'
## Task
`scripts/calibrate_loss_normalization.py` currently has **stub loss functions** (lines 37-83). Replace them with imports from `train_baseline_unet.py`.

## Mapping
| Stub | Real class | Location in train_baseline_unet.py |
|------|-----------|-----------------------------------|
| `base_loss` | `F.mse_loss` or `F.l1_loss` | Standard PyTorch |
| `gradient_loss` | `GradientLoss3D` | ~lines 420-470 |
| `structure_weighted_loss` | `StructureWeightedLoss` | ~lines 600+ |
| `asymmetric_ptv_loss` | `AsymmetricPTVLoss` | ~lines 730+ |
| `dvh_aware_loss` | `DVHAwareLoss` | ~lines 500-600 |

## Why this matters
Running calibration with stubs produces meaningless `initial_log_sigma` values. The real losses have different magnitude profiles (3D Sobel vs simple L1 diff, etc.).

## Acceptance criteria
- [ ] All 5 stubs replaced with real loss function imports
- [ ] Calibration runs without errors on real data
- [ ] Output `loss_normalization_calib.json` has plausible values
BODY
)" \
    "phase/2-combined,type/pipeline" \
    "$MS2"

create_issue \
    "Extend UncertaintyWeightedLoss to accept per-component initial_log_sigma" \
    "$(cat <<'BODY'
## Task
`scripts/uncertainty_loss.py` currently takes a single `initial_log_sigma: float` for all components. Extend it to accept per-component initialization.

## Current interface
```python
UncertaintyWeightedLoss(
    loss_names=["base", "gradient", "structure", "asymmetric", "dvh"],
    initial_log_sigma=0.0  # same value for all
)
```

## Target interface
```python
UncertaintyWeightedLoss(
    loss_names=["base", "gradient", "structure", "asymmetric", "dvh"],
    initial_log_sigma={"base": -0.23, "gradient": -1.54, "structure": -0.89, ...}
    # Also accept float for backward compatibility (applies to all)
)
```

## Why
The calibration script produces per-component values. Initializing all sigmas identically when losses differ by 10x in magnitude defeats the purpose of calibration.

## Acceptance criteria
- [ ] Accepts `Dict[str, float]` or `float` for `initial_log_sigma`
- [ ] Backward compatible (float still works)
- [ ] Values from calibration JSON load cleanly
BODY
)" \
    "phase/2-combined" \
    "$MS2"

create_issue \
    "Integrate combined 5-component loss into train_baseline_unet.py" \
    "$(cat <<'BODY'
## Task
Wire up all 5 loss components under `UncertaintyWeightedLoss` in the main training script.

## Components
1. Base MSE/L1 loss
2. Gradient loss (3D Sobel, pilot weight 0.1)
3. Structure-weighted loss (2x PTV, 1.5x OAR boundary, 0.1x background)
4. Asymmetric PTV loss (underdose >> overdose)
5. DVH-aware loss (D95, Dmean/Vx compliance)

## Implementation
- Add `--loss_mode uncertainty_weighted` CLI flag (or similar)
- Load calibrated `initial_log_sigma` from JSON
- Log per-component raw losses, weighted losses, and sigma values to TensorBoard
- Verify training stability (sigma values stay in 0.1-3.0 range)

## Depends on
- Calibration stubs replaced (#11)
- Per-component sigma (#12)

## Acceptance criteria
- [ ] All 5 losses compute and backprop without errors
- [ ] TensorBoard shows per-component loss curves + sigma evolution
- [ ] Training completes 10 epochs without NaN/explosion
- [ ] Sigma values stabilize in reasonable range
BODY
)" \
    "phase/2-combined,type/experiment" \
    "$MS2"

create_issue \
    "Run combined loss experiment (3 seeds) on 100+ cases" \
    "$(cat <<'BODY'
## Task
The main Phase 2 experiment: combined 5-component loss with uncertainty weighting.

## Protocol
1. Run `scripts/calibrate_loss_normalization.py` on production dataset → `loss_normalization_calib.json`
2. Train with 3 seeds: 42, 123, 789
3. At ~50% training (~epoch 125), run 5-epoch ablation sweep (each loss on/off individually)
4. Complete training to 250 epochs
5. Run test-set inference and full clinical evaluation for each seed

## Metrics to report
- MAE (Gy): mean ± std across seeds
- Global Gamma 3%/3mm: mean ± std (diagnostic, expect 75-88%)
- PTV-region Gamma 3%/3mm: mean ± std (target >90%)
- PTV70 D95: mean ± std (target ≥66.5 Gy)
- PTV56 D95: mean ± std (target ≥53.2 Gy)
- Per-structure DVH compliance rate
- Paired Wilcoxon test: combined vs best single-loss

## Depends on
- Combined loss integration (#13)
- Clinical evaluation framework (Phase 1)
- 100+ case dataset (Phase 0)

## Full experiment documentation per CLAUDE.md protocol required.

## Acceptance criteria
- [ ] 3 seeds complete without errors
- [ ] Per-component sigma stable (0.1-3.0 range)
- [ ] Ablation sweep completed and documented
- [ ] Mean ± std for all metrics
- [ ] Wilcoxon test p-values computed
- [ ] Publication-ready notebook with all 10 sections
- [ ] EXPERIMENTS_INDEX.md updated
BODY
)" \
    "phase/2-combined,type/experiment,priority/critical" \
    "$MS2"

# ---- PHASE 3 ISSUES ----

create_issue \
    "Analyze Phase 2 results and determine Phase 3 direction" \
    "$(cat <<'BODY'
## Task
Based on Phase 2 combined-loss results, determine which Phase 3 path to take.

## Decision tree
- **If DVH compliance is close but not there** → Tune loss weights (GradNorm or sequential ±20% ablation)
- **If architecture is the bottleneck** → Try attention U-Net or deeper network
- **If data diversity is limiting** → Add augmentation (geometric, OAR contour perturbation, constraint perturbation)
- **If results are publication-ready** → Skip to publication prep

## Acceptance criteria
- [ ] Phase 2 results analyzed against clinical targets
- [ ] Decision documented as GitHub issue with `type/decision` label
- [ ] Next experiment planned based on analysis
BODY
)" \
    "phase/3-iterate,type/decision" \
    "$MS3"

create_issue \
    "Write failure case report (bottom 10% by Gamma/DVH)" \
    "$(cat <<'BODY'
## Task
Identify and characterize the worst-performing cases from the combined-loss experiment.

## Analysis
- Identify bottom 10% of test cases by PTV-region Gamma AND DVH compliance
- Categorize failure modes:
  - Anatomical complexity (e.g., unusual prostate/SV geometry)
  - Contour ambiguity (e.g., inconsistent structure delineation)
  - Constraint conflicts (e.g., PTV70/bladder overlap)
  - Data quality issues
- Propose case-specific mitigations or model improvements

## Publication value
Honest failure analysis significantly strengthens the manuscript. Reviewers expect it.

## Depends on
- Phase 2 experiment completed (#14)

## Acceptance criteria
- [ ] Bottom 10% cases identified and visualized
- [ ] Failure modes categorized
- [ ] Mitigations proposed
- [ ] Publication-ready failure analysis section drafted
BODY
)" \
    "phase/3-iterate,type/publication" \
    "$MS3"

create_issue \
    "Prepare code release (anonymize, freeze deps, DOI)" \
    "$(cat <<'BODY'
## Task
Prepare the codebase for public release alongside the publication.

## Steps
- [ ] Remove internal paths and institution identifiers from all scripts
- [ ] Remove or generalize any hard-coded data paths
- [ ] Freeze dependencies: export exact conda environment used for final results
- [ ] Write minimal README for external users (setup, reproduce key results)
- [ ] Create GitHub Release with version tag (e.g., v1.0.0-paper)
- [ ] Register DOI via Zenodo (linked to GitHub release)
- [ ] Verify a fresh clone + env setup + inference works end-to-end

## Depends on
- Final experiment results locked

## Acceptance criteria
- [ ] Clean clone + setup + inference passes on fresh machine
- [ ] No PHI, internal paths, or credentials in any committed file
- [ ] DOI registered and linkable from manuscript
BODY
)" \
    "phase/3-iterate,type/publication" \
    "$MS3"

create_issue \
    "Draft and submit Medical Physics manuscript" \
    "$(cat <<'BODY'
## Task
Write and submit the primary manuscript.

## Framing
"Loss-function engineering for clinically acceptable prostate VMAT dose prediction"

## Target venue
**Medical Physics** (primary). Fallback: PMB if emphasizing physics/novelty, JACMP for clinical implementation.

## Required sections
- Introduction: clinical motivation, literature gap
- Methods: architecture, loss components, uncertainty weighting, evaluation framework
- Results: ablation study (5 individual + combined), multi-seed statistics, clinical acceptability rates
- Discussion: failure analysis, comparison to literature benchmarks, limitations
- External validation statement (single-institution limitation + multi-site plan)

## Depends on
- Phase 2 results finalized
- Failure case report (#16)
- Physician preference study (#10, if completed by submission)

## Acceptance criteria
- [ ] Manuscript drafted with all sections
- [ ] All figures publication-ready (300 DPI, PDF + PNG)
- [ ] Statistical analysis complete (Wilcoxon, mean±std)
- [ ] Co-author review cycle completed
- [ ] Submitted to Medical Physics
BODY
)" \
    "phase/3-iterate,type/publication,priority/high" \
    "$MS3"

# ---- PARKING LOT / BACKBURNER ISSUES ----

create_issue \
    "[Backburner] Adversarial loss (PatchGAN) for dose edge sharpness" \
    "$(cat <<'BODY'
## Idea
Add a PatchGAN discriminator to sharpen dose gradient edges, particularly at PTV boundaries.

## When to try
Only if combined loss results show blurry dose edges despite gradient loss.

## Considerations
- PatchGAN adds training complexity (discriminator + generator alternation)
- VRAM overhead (~4-6 GB additional)
- Risk of mode collapse in medical context
- Literature precedent: used in some CT-to-dose papers with mixed results
BODY
)" \
    "type/backburner,type/if-stuck" \
    ""

create_issue \
    "[Backburner] Flow Matching / Consistency Models for sampling diversity" \
    "$(cat <<'BODY'
## Idea
Replace or augment the deterministic U-Net with a generative model that can sample multiple plausible dose distributions per case (rather than averaging).

## When to try
Only if the averaging problem (low global Gamma from mean-regression) persists after combined loss optimization and is identified as the primary bottleneck.

## Options
- Flow Matching (simpler than DDPM, single-step generation)
- Consistency Models (fast sampling from diffusion framework)

## Why not DDPM
DDPM was already tried and abandoned (see decision log). Structural mismatch: more denoising steps = worse. Flow Matching avoids this.
BODY
)" \
    "type/backburner,type/if-stuck" \
    ""

create_issue \
    "[Backburner] Architecture alternatives: nnU-Net, Swin-UNETR" \
    "$(cat <<'BODY'
## Idea
Try proven medical segmentation architectures adapted for dose prediction.

## Options
- **nnU-Net:** Auto-configuring U-Net. Would need adaptation from segmentation to regression.
- **Swin-UNETR:** Transformer-based. Better long-range context but higher VRAM.
- **Lightweight cross-attention / Swin blocks in bottleneck only:** VRAM-conscious middle ground.

## When to try
Only if Phase 3 analysis identifies architecture as the bottleneck (not loss design or data).

## VRAM constraints
RTX 3090 = 24 GB. Current BaselineUNet3D uses ~8 GB. Swin-UNETR would likely use 16-20 GB at 128³ patches.
BODY
)" \
    "type/backburner,type/if-stuck" \
    ""

create_issue \
    "[Backburner] Ensemble predictions from existing models" \
    "$(cat <<'BODY'
## Idea
Average predictions from multiple existing trained models (e.g., structure-weighted + DVH-aware + gradient loss) as a quick experiment.

## Why
- Zero additional training cost
- Ensembles often improve calibration and reduce variance
- Could be done on pilot data as a quick feasibility test

## When to try
Quick experiment that could be done anytime. Low cost, moderate potential benefit.
BODY
)" \
    "type/backburner,type/experiment,priority/low" \
    ""

create_issue \
    "[Backburner] OAR contour perturbation augmentation" \
    "$(cat <<'BODY'
## Idea
Apply small random shifts to structure boundaries during training to simulate inter-observer contouring variability.

## Rationale
Contouring variability is a real clinical source of diversity. Training with perturbed contours makes the model robust to this. More clinically motivated than generic elastic deformation alone.

## Implementation
- Apply small (1-3mm) random translations/deformations to individual structure masks
- Recompute SDFs after perturbation
- Apply independently per training sample

## When to try
Phase 3, if data diversity is identified as limiting factor.
BODY
)" \
    "type/backburner,phase/3-iterate" \
    ""

# ---- DECISION RECORDS ----

create_issue \
    "[Decision] Paper framing: loss-function engineering for clinical VMAT dose prediction" \
    "$(cat <<'BODY'
## Decision (2026-02-17)
Paper framing: **"Loss-function engineering for clinically acceptable prostate VMAT dose prediction"**

## Rationale
External review (Grok, 2026-02-17) independently validated project direction and suggested this framing. Pilot has 5 loss variants with clean ablation data — with 100+ cases and combined loss results, this is a strong Medical Physics submission.

## Alternatives considered
- Architecture-focused paper → less novel, pilot shows architecture is not the bottleneck
- Multi-model comparison → less focused, DDPM was abandoned
- Pure clinical validation → insufficient without the loss engineering story

## Status: ACCEPTED
BODY
)" \
    "type/decision" \
    ""

create_issue \
    "[Decision] Uncertainty weighting (Kendall 2018), NOT grid search, for loss combination" \
    "$(cat <<'BODY'
## Decision (2026-02-17)
Use **Uncertainty Weighting (Kendall et al. 2018)** for combining loss components. NOT grid search.

## Approach
1. Normalize losses first (run each 10-20 epochs, record mean, divide by mean → all start ~1.0)
2. Learn one scalar σ per loss; weights become 1/(2σ²) automatically
3. Fallback: GradNorm if any single loss dominates

## Rationale
- Stable, cheap, proven in medical multi-task learning
- One learnable parameter per loss instead of exponential grid search
- External review consensus (2026-02-17)

## Status: ACCEPTED
BODY
)" \
    "type/decision" \
    ""

create_issue \
    "[Decision] Shift primary metric from global Gamma to DVH + PTV-region Gamma" \
    "$(cat <<'BODY'
## Decision (2026-02-13)
**Primary optimization targets:** DVH compliance + PTV-region Gamma + gradient realism.
**Global Gamma:** Diagnostic only — track but do NOT optimize for it.

## Rationale
- Global Gamma penalizes valid low-dose diversity (multiple acceptable solutions exist)
- PTV Gamma (41.5%) is much higher than overall (31.2%) — model IS accurate where it matters
- DVH compliance + physical realism = what clinicians actually evaluate
- Pilot 28-31% global Gamma is expected at n=23 (literature: 75-85% at n=50-100)

## Status: ACCEPTED
BODY
)" \
    "type/decision" \
    ""

create_issue \
    "[Decision] DDPM abandoned — structural mismatch, no benefit over U-Net" \
    "$(cat <<'BODY'
## Decision (2026-01-20)
**DDPM is not recommended.** Abandoned as a dead end.

## Rationale
- Matches baseline U-Net but does NOT beat it on any metric
- Structural mismatch: more denoising steps = worse (opposite of expected behavior)
- Near-zero sample variability → not actually functioning as a generative model
- Added complexity (23 GB VRAM, complex sampling) with no benefit
- May revisit only with physics-bounded approach (region-aware noise) if simpler methods plateau

## Status: ACCEPTED (archived)
BODY
)" \
    "type/decision" \
    ""

create_issue \
    "[Decision] Publication target: Medical Physics journal" \
    "$(cat <<'BODY'
## Decision (2026-02-17)
**Primary target: Medical Physics** — single comprehensive paper.

## Rationale
- Med Phys is the ideal venue for a ~100-case, 5-ablation, combined-results loss-engineering paper
- Alternatives: PMB (if emphasizing physics/novelty), JACMP (follow-up clinical implementation study)
- Don't split prematurely — one strong paper first
- Second "clinical validation" paper (with physician rankings + deliverability) later if warranted

## Status: ACCEPTED
BODY
)" \
    "type/decision" \
    ""

# -------------------------------------------------------------------
# 4. SUMMARY
# -------------------------------------------------------------------
echo ""
echo "============================================="
echo "SETUP COMPLETE"
echo "============================================="
echo ""
echo "Labels created/verified."
echo "Milestones created: Phase 0, Phase 1, Phase 2, Phase 3"
echo "Issues created for all roadmap items."
echo ""
echo "Next steps:"
echo "  1. View your project board: gh project list --owner wrockey"
echo "  2. Or create one:  gh project create --owner wrockey --title 'VMAT Diffusion Roadmap'"
echo "  3. Link issues to the project board in the GitHub web UI"
echo "     (or use: gh project item-add <project-number> --owner wrockey --url <issue-url>)"
echo ""
echo "Tip: To see all issues by phase:"
echo "  gh issue list --label phase/0-setup"
echo "  gh issue list --label phase/1-eval"
echo "  gh issue list --label phase/2-combined"
echo "  gh issue list --label phase/3-iterate"
echo "  gh issue list --label type/backburner"
echo "  gh issue list --label type/decision"
