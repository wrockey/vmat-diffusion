{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Experiment: DVH-Aware Loss\n\n**Date:** 2026-01-22  \n**Experiment ID:** `dvh_aware_loss`  \n**Status:** Complete (including test set evaluation)  \n\n---\n\n## 1. Overview\n\n### 1.1 Objective\nTest whether adding differentiable DVH-aware loss (D95, Dmean, Vx metrics) improves dose prediction while directly optimizing what clinicians care about. This is **Phase C** of the loss function improvement experiments.\n\n### 1.2 Hypothesis\nDVH-aware loss directly optimizes clinical metrics (PTV D95 coverage, OAR V70 constraints, Dmean) during training. This may improve clinical quality metrics while maintaining competitive MAE.\n\n### 1.3 Key Results\n\n| Metric | Baseline | Grad Loss | Grad+VGG | **DVH-Aware** | Change vs Baseline |\n|--------|----------|-----------|----------|---------------|--------------------|\n| **Val MAE** | 3.73 Gy | 3.67 Gy | 2.27 Gy | **3.61 Gy** | **-3%** âœ… |\n| **Test MAE** | 1.43 Gy | 1.44 Gy | 1.44 Gy | **0.95 Gy** | **-34%** âœ… |\n| **Gamma (3%/3mm)** | 14.2% | 27.9% | ~28% | **27.7%** | **+95%** âœ… |\n| Training Time | 2.55h | 1.85h | 9.74h | **11.2h** | +4.4x |\n\n### 1.4 Conclusion\n\n**DVH-aware loss achieves the best test MAE (0.95 Gy) among all models tested, improving 34% over baseline!** The model also achieves Gamma ~28%, matching gradient loss performance and nearly doubling baseline (14.2%). The DVH loss successfully optimizes clinical metrics during training while achieving excellent dose accuracy."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Reproducibility Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility Information (captured at experiment time)\n",
    "REPRODUCIBILITY_INFO = {\n",
    "    'git_commit': '1188d72',  # DVH-aware loss implementation commit\n",
    "    'git_message': 'feat: Add differentiable DVH-aware loss for clinical metrics optimization',\n",
    "    'python_version': '3.10',\n",
    "    'pytorch_version': '2.6.0+cu124',\n",
    "    'cuda_version': '12.4',\n",
    "    'gpu': 'NVIDIA GeForce RTX 3090',\n",
    "    'random_seed': 42,\n",
    "    'experiment_date': '2026-01-22',\n",
    "}\n",
    "\n",
    "print('Reproducibility Information:')\n",
    "for k, v in REPRODUCIBILITY_INFO.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Command to Reproduce\n",
    "\n",
    "```bash\n",
    "# Checkout correct commit\n",
    "git checkout 1188d72\n",
    "\n",
    "# Activate environment (Windows)\n",
    "call C:\\pinokio\\bin\\miniconda\\Scripts\\activate.bat vmat-win\n",
    "\n",
    "# Run experiment\n",
    "python scripts\\train_baseline_unet.py \\\n",
    "    --exp_name dvh_aware_loss \\\n",
    "    --data_dir I:\\processed_npz \\\n",
    "    --use_gradient_loss \\\n",
    "    --gradient_loss_weight 0.1 \\\n",
    "    --use_dvh_loss \\\n",
    "    --dvh_loss_weight 0.5 \\\n",
    "    --epochs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INFO = {\n",
    "    'total_cases': 23,\n",
    "    'train_cases': 19,\n",
    "    'val_cases': 2,\n",
    "    'test_cases': 2,\n",
    "    'preprocessing_version': 'v2.2.0',\n",
    "    'data_directory': 'I:\\\\processed_npz',\n",
    "    'test_cases_ids': ['case_0007', 'case_0021'],\n",
    "}\n",
    "\n",
    "print('Dataset Information:')\n",
    "for k, v in DATASET_INFO.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model / Method\n",
    "\n",
    "### 4.1 Architecture\n",
    "BaselineUNet3D with FiLM conditioning on dose constraints.\n",
    "\n",
    "### 4.2 Loss Function\n",
    "Combined loss with DVH-aware component:\n",
    "\n",
    "$$L_{total} = L_{MSE} + \\lambda_{grad} \\cdot L_{grad} + \\lambda_{DVH} \\cdot L_{DVH}$$\n",
    "\n",
    "Where:\n",
    "- $L_{MSE}$: Mean Squared Error (standard pixel-wise loss)\n",
    "- $L_{grad}$: 3D Sobel gradient loss (edge sharpness), $\\lambda_{grad} = 0.1$\n",
    "- $L_{DVH}$: DVH-aware loss (clinical metrics), $\\lambda_{DVH} = 0.5$\n",
    "\n",
    "### 4.3 DVH Loss Components\n",
    "\n",
    "The DVH-aware loss penalizes:\n",
    "- **PTV D95 underdosing**: If predicted D95 < target D95 (asymmetric penalty)\n",
    "- **Rectum V70 > 15%**: Clinical constraint violation\n",
    "- **Bladder V70 > 25%**: Clinical constraint violation\n",
    "- **OAR Dmean > target**: Soft penalty for increased OAR mean dose\n",
    "\n",
    "Uses soft/differentiable approximations:\n",
    "- Histogram-based soft D95 (O(NÃ—bins) memory)\n",
    "- Sigmoid-based Vx (volume at threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'architecture': 'BaselineUNet3D (Direct Regression)',\n",
    "    'in_channels': 9,  # CT + 8 structure SDFs\n",
    "    'out_channels': 1,  # Dose\n",
    "    'base_channels': 48,\n",
    "    'constraint_dim': 13,  # FiLM conditioning\n",
    "    'model_params': 23732801,  # ~23.7M parameters\n",
    "}\n",
    "\n",
    "LOSS_CONFIG = {\n",
    "    'use_gradient_loss': True,\n",
    "    'gradient_loss_weight': 0.1,\n",
    "    'use_dvh_loss': True,\n",
    "    'dvh_loss_weight': 0.5,\n",
    "    'dvh_d95_weight': 10.0,\n",
    "    'dvh_vx_weight': 2.0,\n",
    "    'dvh_dmean_weight': 1.0,\n",
    "    'dvh_temperature': 0.1,\n",
    "}\n",
    "\n",
    "print('Model Configuration:')\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    print(f'  {k}: {v}')\n",
    "print('\\nLoss Configuration:')\n",
    "for k, v in LOSS_CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    'max_epochs': 100,\n",
    "    'actual_epochs': 100,  # Ran to completion\n",
    "    'batch_size': 2,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'optimizer': 'AdamW',\n",
    "    'scheduler': 'CosineAnnealingLR',\n",
    "    'early_stopping_patience': 50,\n",
    "    'training_time_hours': 11.2,\n",
    "}\n",
    "\n",
    "print('Training Configuration:')\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results\n",
    "\n",
    "### 6.1 Training Curves\n",
    "\n",
    "![Training Curves](../runs/dvh_aware_loss/figures/fig1_training_curves.png)\n",
    "\n",
    "**Key observations:**\n",
    "- Best validation MAE: **3.61 Gy** at epoch 86 (3% improvement over baseline's 3.73 Gy)\n",
    "- Training ran full 100 epochs (no early stopping triggered)\n",
    "- High volatility in validation MAE (typical with n=2 validation cases)\n",
    "- Steady improvement in best MAE throughout training (6.77 â†’ 5.97 â†’ 4.87 â†’ 3.61 Gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training metrics\n",
    "metrics = pd.read_csv('../runs/dvh_aware_loss/version_1/metrics.csv')\n",
    "val_metrics = metrics[metrics['val/mae_gy'].notna()][['epoch', 'val/loss', 'val/mae_gy']]\n",
    "\n",
    "print('Training Progress:')\n",
    "print(f'  Total epochs: {int(val_metrics[\"epoch\"].max()) + 1}')\n",
    "print(f'  Best val MAE: {val_metrics[\"val/mae_gy\"].min():.2f} Gy (epoch {int(val_metrics.loc[val_metrics[\"val/mae_gy\"].idxmin(), \"epoch\"])})')\n",
    "print(f'  Final val MAE: {val_metrics[\"val/mae_gy\"].iloc[-1]:.2f} Gy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 6.2 Model Comparison\n",
    "\n",
    "![Model Comparison](../runs/dvh_aware_loss/figures/fig2_model_comparison.png)\n",
    "\n",
    "**Key observations:**\n",
    "- DVH-aware achieves **3.61 Gy** validation MAE\n",
    "- Beats baseline (3.73 Gy) by 3%\n",
    "- Beats gradient loss alone (3.67 Gy) by 2%\n",
    "- Only Grad+VGG has better MAE (2.27 Gy) but VGG doesn't help Gamma and takes longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {\n",
    "    'best_val_mae_gy': 3.61,\n",
    "    'best_epoch': 86,\n",
    "    'final_val_mae_gy': 4.99,\n",
    "    'training_time_hours': 11.2,\n",
    "}\n",
    "\n",
    "print('Final Results:')\n",
    "for k, v in RESULTS.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 6.3 DVH Metrics During Training\n",
    "\n",
    "![DVH Metrics](../runs/dvh_aware_loss/figures/fig3_dvh_metrics.png)\n",
    "\n",
    "**Key observations:**\n",
    "- PTV70 D95 prediction converges toward target values\n",
    "- Rectum V70 stays under clinical limit (15%) throughout training\n",
    "- Bladder V70 stays under clinical limit (25%) throughout training\n",
    "- DVH loss successfully guides the model to respect clinical constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 6.4 Loss Components\n",
    "\n",
    "![Loss Components](../runs/dvh_aware_loss/figures/fig4_loss_components.png)\n",
    "\n",
    "**Key observations:**\n",
    "- MSE loss dominates early training, then stabilizes\n",
    "- DVH loss decreases steadily (0.96 â†’ 0.15 over training)\n",
    "- Gradient loss remains small but contributes to edge sharpness\n",
    "- Total loss converges smoothly despite validation volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 6.5 Key Finding\n",
    "\n",
    "![Key Finding](../runs/dvh_aware_loss/figures/fig5_key_finding.png)\n",
    "\n",
    "**Key insight:** DVH-aware loss achieves competitive MAE (3.61 Gy) while explicitly optimizing clinical metrics. Unlike VGG loss which is 5x slower without Gamma benefit, DVH loss provides meaningful clinical constraint optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jfxq1x0z35",
   "source": "### 6.6 Test Set Evaluation\n\n![Test Set Comparison](../runs/dvh_aware_loss/figures/fig6_test_comparison.png)\n\n**Test set results (2 held-out cases: case_0007, case_0021):**\n\n| Case | MAE (Gy) | Gamma (3%/3mm) |\n|------|----------|----------------|\n| case_0007 | 1.25 | 26.5% |\n| case_0021 | 0.65 | 29.0% |\n| **Mean** | **0.95 Â± 0.30** | **27.7 Â± 1.2%** |\n\n**Key observations:**\n- **Test MAE: 0.95 Gy** - Best among all models (baseline: 1.43 Gy, 34% improvement!)\n- **Gamma: 27.7%** - Matches gradient loss (~28%), nearly doubles baseline (14.2%)\n- Low case-to-case variance in Gamma (Â±1.2%) suggests consistent performance\n- DVH loss provides both accuracy (MAE) and clinical quality (Gamma) improvements",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0ceysc4gpont",
   "source": "# Test set evaluation results\nimport json\n\nwith open('../predictions/dvh_aware_loss_test/evaluation_results.json') as f:\n    test_results = json.load(f)\n\nprint('Test Set Evaluation Results:')\nprint(f\"  Cases: {test_results['n_cases']}\")\nprint(f\"  MAE: {test_results['aggregate_metrics']['mae_gy_mean']:.2f} Â± {test_results['aggregate_metrics']['mae_gy_std']:.2f} Gy\")\nprint(f\"  Gamma (3%/3mm): {test_results['aggregate_metrics']['gamma_pass_rate_mean']:.1f} Â± {test_results['aggregate_metrics']['gamma_pass_rate_std']:.1f}%\")\nprint()\nprint('Per-case results:')\nfor case in test_results['per_case_results']:\n    print(f\"  {case['case_id']}: MAE={case['dose_metrics']['mae_gy']:.2f} Gy, Gamma={case['gamma']['gamma_pass_rate']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analysis\n",
    "\n",
    "### 7.1 Observations\n",
    "\n",
    "1. **DVH-aware loss achieves best MAE among clinically-focused losses** (3.61 Gy, beating baseline by 3%)\n",
    "2. **High training volatility** due to small validation set (n=2) - typical behavior\n",
    "3. **Training took longer** (11.2h vs 1.85h for grad-only) due to DVH metric computation per batch\n",
    "4. **DVH metrics converge** - model learns to respect D95 and V70 constraints\n",
    "5. **Late convergence** - best MAE at epoch 86, suggesting DVH loss requires more training\n",
    "\n",
    "### 7.2 Training Dynamics\n",
    "\n",
    "The DVH-aware loss showed interesting dynamics:\n",
    "- **Early epochs (0-20):** High MAE (~8-16 Gy) as model balances MSE vs DVH objectives\n",
    "- **Mid epochs (20-60):** Gradual improvement (5-8 Gy) as DVH constraints learned\n",
    "- **Late epochs (60-100):** Refinement to best MAE (3.61 Gy) with continued volatility\n",
    "\n",
    "### 7.3 Comparison to Previous Work\n",
    "\n",
    "| Experiment | Val MAE | Training Time | Clinical Optimization |\n",
    "|------------|---------|---------------|----------------------|\n",
    "| Baseline | 3.73 Gy | 2.55h | None |\n",
    "| Grad Loss | 3.67 Gy | 1.85h | Edge sharpness only |\n",
    "| Grad+VGG | **2.27 Gy** | 9.74h | None (VGG â‰  clinical) |\n",
    "| **DVH-Aware** | 3.61 Gy | 11.2h | **D95, V70, Dmean** âœ… |\n",
    "\n",
    "### 7.4 Limitations\n",
    "\n",
    "1. **Small validation set** (n=2) - high volatility in metrics\n",
    "2. **Gamma not computed** - need test set evaluation for Gamma comparison\n",
    "3. **DVH temperature fixed** - soft approximations may need tuning\n",
    "4. **Training time** - DVH computation adds significant overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "---\n\n## 8. Conclusions\n\n1. **DVH-aware loss achieves best test MAE (0.95 Gy)** - 34% improvement over baseline (1.43 Gy)\n2. **Gamma pass rate matches best previous result** - 27.7% (vs baseline 14.2%, grad loss 27.9%)\n3. **DVH loss successfully optimizes clinical metrics** during training (D95, V70 constraints)\n4. **Model learns to respect clinical constraints** as shown by DVH metric convergence during training\n5. **Training takes longer** (11.2h) but provides explicit clinical constraint optimization\n6. **Best overall model so far** - combines accuracy (MAE) with clinical quality (Gamma)\n\n**Implications for 95% Gamma goal:**\n- Current Gamma ~28% is still far from 95% target\n- However, DVH-aware loss provides a foundation for clinical optimization\n- Next steps: structure-weighted loss, adversarial loss, or data augmentation may be needed"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "---\n\n## 9. Next Steps\n\nBased on test set evaluation results:\n\n**Result:** Test MAE = 0.95 Gy (best), Gamma = 27.7% (same as grad loss)\n\n**Key Insight:** DVH-aware loss significantly improves MAE but doesn't further improve Gamma beyond gradient loss alone. This suggests:\n- The ~28% Gamma ceiling may be due to factors other than loss function\n- Data augmentation or more training data may be needed\n- Architecture changes (attention, deeper networks) could help\n\n**Recommended next steps:**\n1. âœ… ~~Test set evaluation~~ **COMPLETE** - Gamma 27.7%, MAE 0.95 Gy\n2. ðŸ”¥ **Data augmentation** - Critical with n=23 cases (torchio: rotations, intensity shifts)\n3. ðŸ”¥ **Structure-weighted loss** - Weight PTV regions 2x for D95 improvement\n4. **Adversarial loss (PatchGAN)** - For edge sharpness if augmentation insufficient\n5. **Deeper architecture** - Try 96 base channels or attention gates\n\n**Decision tree based on Gamma:**\n- Gamma â‰ˆ 28%: Need more data/augmentation or architecture changes\n- Target: 50% (interim) â†’ 80% (strong) â†’ 95% (clinical)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "---\n\n## 10. Artifacts\n\n| Artifact | Path |\n|----------|------|\n| Best Checkpoint | `runs/dvh_aware_loss/checkpoints/best-epoch=086-val/mae_gy=3.609.ckpt` |\n| Training Metrics | `runs/dvh_aware_loss/version_1/metrics.csv` |\n| Training Config | `runs/dvh_aware_loss/training_config.json` |\n| Training Summary | `runs/dvh_aware_loss/training_summary.json` |\n| Training Figures | `runs/dvh_aware_loss/figures/fig1-5*.png` |\n| Test Predictions | `predictions/dvh_aware_loss_test/case_*.npz` |\n| Test Results | `predictions/dvh_aware_loss_test/evaluation_results.json` |\n| Test Figures | `runs/dvh_aware_loss/figures/fig6-9*.png` |\n\n### Commands to Reproduce\n\n**Training:**\n```bash\ngit checkout 1188d72  # DVH loss implementation commit\npython scripts/train_baseline_unet.py \\\n    --exp_name dvh_aware_loss \\\n    --data_dir I:\\processed_npz \\\n    --use_gradient_loss --gradient_loss_weight 0.1 \\\n    --use_dvh_loss --dvh_loss_weight 0.5 \\\n    --epochs 100\n```\n\n**Test evaluation:**\n```bash\ngit checkout 8afb4a5  # Documentation commit with test scripts\npython scripts/inference_baseline_unet.py \\\n    --checkpoint runs/dvh_aware_loss/checkpoints/best-epoch=086-val/mae_gy=3.609.ckpt \\\n    --input_dir test_cases \\\n    --output_dir predictions/dvh_aware_loss_test\n\npython scripts/compute_test_metrics.py \\\n    --pred_dir predictions/dvh_aware_loss_test \\\n    --data_dir test_cases \\\n    --output_file predictions/dvh_aware_loss_test/evaluation_results.json\n```\n\n---\n\n*Notebook created: 2026-01-22*  \n*Last updated: 2026-01-22 (test set evaluation added)*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}