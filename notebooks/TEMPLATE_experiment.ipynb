{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment: [TITLE]\n",
    "\n",
    "**Date:** YYYY-MM-DD  \n",
    "**Experiment ID:** `experiment_name`  \n",
    "**Status:** In Progress / Complete / Failed  \n",
    "**Type:** Training / Analysis  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1 Objective\n",
    "\n",
    "[What are we testing? What hypothesis? What is the clinical motivation?]\n",
    "\n",
    "### 1.2 Key Results\n",
    "\n",
    "| Metric | This Experiment | Baseline | Change |\n",
    "|--------|----------------|----------|--------|\n",
    "| MAE (Gy) | X.XX ± X.XX | X.XX ± X.XX | +/-X% |\n",
    "| Gamma 3%/3mm | XX.X ± X.X% | XX.X ± X.X% | +/-X% |\n",
    "| PTV70 D95 Gap (Gy) | X.XX ± X.XX | X.XX ± X.XX | +/-X Gy |\n",
    "\n",
    "### 1.3 Conclusion\n",
    "\n",
    "[1-2 sentence summary: what did we learn and what does it mean clinically?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. What Changed\n",
    "\n",
    "Compared to **[prior experiment name]** (`git_hash`), this experiment changes:\n",
    "\n",
    "| Parameter | Prior Value | This Experiment |\n",
    "|-----------|------------|------------------|\n",
    "| [changed param] | [old] | [new] |\n",
    "\n",
    "**Everything else is identical.** If more than one variable changed, justify why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-capture reproducibility info\n",
    "REPRODUCIBILITY = {\n",
    "    'git_commit': subprocess.getoutput('git rev-parse HEAD'),\n",
    "    'git_message': subprocess.getoutput('git log -1 --format=\"%s\"'),\n",
    "    'git_dirty': subprocess.getoutput('git status --porcelain') != '',\n",
    "    'python_version': f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}',\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n",
    "    'random_seed': 42,\n",
    "    'experiment_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "}\n",
    "\n",
    "print('Reproducibility Information:')\n",
    "for k, v in REPRODUCIBILITY.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "if REPRODUCIBILITY['git_dirty']:\n",
    "    print('\\n  WARNING: Uncommitted changes present! Commit before running.')\n",
    "\n",
    "# Verify environment snapshot exists\n",
    "EXP_NAME = 'FILL_IN'  # <-- SET THIS\n",
    "env_snapshot = Path(f'../runs/{EXP_NAME}/environment_snapshot.txt')\n",
    "if env_snapshot.exists():\n",
    "    print(f'\\n  Environment snapshot: {env_snapshot}')\n",
    "else:\n",
    "    print(f'\\n  WARNING: No environment snapshot found. Run: conda list --export > {env_snapshot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Command to Reproduce\n",
    "\n",
    "```bash\n",
    "# 1. Checkout exact code\n",
    "git checkout <COMMIT_HASH>\n",
    "\n",
    "# 2. Activate environment\n",
    "conda activate vmat-diffusion\n",
    "\n",
    "# 3. Train\n",
    "python scripts/train_baseline_unet.py \\\n",
    "    --exp_name <NAME> \\\n",
    "    --data_dir ~/data/processed_npz \\\n",
    "    [FULL FLAGS HERE]\n",
    "\n",
    "# 4. Evaluate\n",
    "python scripts/inference_baseline_unet.py \\\n",
    "    --checkpoint runs/<NAME>/checkpoints/best-*.ckpt \\\n",
    "    --input_dir ~/data/processed_npz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('~/data/processed_npz').expanduser()\n",
    "\n",
    "# Load batch summary for data provenance\n",
    "batch_summary = DATA_DIR / 'batch_summary.json'\n",
    "if batch_summary.exists():\n",
    "    with open(batch_summary) as f:\n",
    "        summary = json.load(f)\n",
    "    print(f'Preprocessing version: {summary.get(\"script_version\", \"unknown\")}')\n",
    "    print(f'Processed date: {summary.get(\"processed_date\", \"unknown\")}')\n",
    "    print(f'Total cases: {summary.get(\"total_cases\", \"unknown\")}')\n",
    "    print(f'Settings: {json.dumps(summary.get(\"settings\", {}), indent=2)}')\n",
    "\n",
    "# Record actual case IDs for each split\n",
    "DATASET = {\n",
    "    'preprocessing_version': 'v2.3.0',\n",
    "    'total_cases': 0,       # FILL IN\n",
    "    'train_case_ids': [],   # FILL IN with actual case IDs\n",
    "    'val_case_ids': [],     # FILL IN\n",
    "    'test_case_ids': [],    # FILL IN\n",
    "}\n",
    "\n",
    "print(f'\\nSplit: {len(DATASET[\"train_case_ids\"])} train / '\n",
    "      f'{len(DATASET[\"val_case_ids\"])} val / '\n",
    "      f'{len(DATASET[\"test_case_ids\"])} test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model & Training Configuration\n",
    "\n",
    "*Skip this section for Analysis-type experiments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load saved training config (auto-generated by training script)\n",
    "config_path = Path(f'../runs/{EXP_NAME}/training_config.json')\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    print('Training Configuration (from training_config.json):')\n",
    "    for k, v in sorted(config.items()):\n",
    "        print(f'  {k}: {v}')\n",
    "else:\n",
    "    print(f'WARNING: {config_path} not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results\n",
    "\n",
    "Figures are generated by `scripts/generate_<exp_name>_figures.py` and loaded here.\n",
    "See CLAUDE.md \"Medical Physics Figure Set\" for the required figures.\n",
    "\n",
    "### 6.1 Training Curves\n",
    "\n",
    "![Training Curves](../runs/EXP_NAME/figures/fig1_training_curves.png)\n",
    "\n",
    "**Caption:** Training loss and validation MAE vs epoch for [experiment name]. [What the reader should observe. How convergence compares to prior experiments.]\n",
    "\n",
    "**Key observations:**\n",
    "- [observation 1]\n",
    "- [observation 2]\n",
    "\n",
    "### 6.2 Dose Colorwash (Representative Case)\n",
    "\n",
    "![Dose Colorwash](../runs/EXP_NAME/figures/fig2_dose_colorwash.png)\n",
    "\n",
    "**Caption:** Predicted (left) vs ground truth (right) dose distribution overlaid on CT for [case_id]. Axial slice through PTV70 centroid. [Clinical interpretation — where does the prediction agree/disagree? Are dose gradients realistic?]\n",
    "\n",
    "### 6.3 Dose Difference Map\n",
    "\n",
    "![Dose Difference](../runs/EXP_NAME/figures/fig3_dose_difference.png)\n",
    "\n",
    "**Caption:** Dose difference (predicted minus ground truth) for [case_id]. Positive values (red) indicate overdose, negative (blue) indicate underdose. [Where are the largest errors? Are they clinically significant?]\n",
    "\n",
    "### 6.4 DVH Comparison\n",
    "\n",
    "![DVH](../runs/EXP_NAME/figures/fig4_dvh_comparison.png)\n",
    "\n",
    "**Caption:** DVH curves for predicted (dashed) vs ground truth (solid) for all structures, [case_id]. [Which structures agree well? Where are the largest discrepancies? Are clinical constraints met?]\n",
    "\n",
    "### 6.5 Gamma Map\n",
    "\n",
    "![Gamma](../runs/EXP_NAME/figures/fig5_gamma_map.png)\n",
    "\n",
    "**Caption:** 3%/3mm gamma index map for [case_id]. Green = pass, red = fail. Overall pass rate: XX.X%. [Where does the model fail? Is failure concentrated in clinically relevant regions?]\n",
    "\n",
    "### 6.6 Per-Case Results\n",
    "\n",
    "![Box Plots](../runs/EXP_NAME/figures/fig6_per_case_boxplots.png)\n",
    "\n",
    "**Caption:** Distribution of MAE, Gamma pass rate, and PTV70 D95 error across N test cases. Box shows IQR, whiskers show 1.5×IQR, dots show outliers. [Are results consistent across cases or do outliers dominate the mean?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load per-case results\n",
    "# results = load_evaluation_results(EXP_NAME)\n",
    "# baseline = load_evaluation_results('baseline_experiment')\n",
    "\n",
    "# Example statistical analysis (fill in with real data)\n",
    "def report_metric(name, values, baseline_values=None):\n",
    "    \"\"\"Report metric with 95% CI and comparison to baseline.\"\"\"\n",
    "    mean = np.mean(values)\n",
    "    ci_low, ci_high = np.percentile(values, [2.5, 97.5])\n",
    "    print(f'{name}: {mean:.2f} (95% CI: [{ci_low:.2f}, {ci_high:.2f}])')\n",
    "    \n",
    "    if baseline_values is not None:\n",
    "        # Paired Wilcoxon signed-rank test\n",
    "        stat, p_value = stats.wilcoxon(values, baseline_values)\n",
    "        diff = np.mean(values) - np.mean(baseline_values)\n",
    "        print(f'  vs baseline: {diff:+.2f} (p={p_value:.4f}, '\n",
    "              f'{\"significant\" if p_value < 0.05 else \"not significant\"} at alpha=0.05)')\n",
    "\n",
    "# report_metric('MAE (Gy)', mae_values, baseline_mae)\n",
    "# report_metric('Gamma 3%/3mm (%)', gamma_values, baseline_gamma)\n",
    "# report_metric('PTV70 D95 Gap (Gy)', d95_values, baseline_d95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Cross-Experiment Comparison\n",
    "\n",
    "Comparison to ALL prior experiments on standardized metrics:\n",
    "\n",
    "| Experiment | MAE (Gy) | Gamma 3%/3mm | PTV70 D95 Gap | Training Time |\n",
    "|------------|----------|--------------|---------------|---------------|\n",
    "| baseline_v23 | X.XX ± X.XX | XX.X ± X.X% | X.XX ± X.XX Gy | X.Xh |\n",
    "| **this_experiment** | **X.XX ± X.XX** | **XX.X ± X.X%** | **X.XX ± X.XX Gy** | **X.Xh** |\n",
    "\n",
    "**Key comparisons:**\n",
    "- [How does this compare to baseline? Is the improvement clinically meaningful?]\n",
    "- [Is the tradeoff (e.g., training time) worth it?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclusions, Limitations, and Next Steps\n",
    "\n",
    "### Conclusions\n",
    "- [Key finding 1]\n",
    "- [Key finding 2]\n",
    "\n",
    "### Limitations\n",
    "- [Limitation 1 — e.g., single seed, limited case count, etc.]\n",
    "- [Limitation 2]\n",
    "\n",
    "### Next Steps\n",
    "- [ ] [What this experiment motivates]\n",
    "- [ ] [Follow-up experiment if results warrant]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Artifacts\n",
    "\n",
    "| Artifact | Path |\n",
    "|----------|------|\n",
    "| Best Checkpoint | `runs/<exp>/checkpoints/best-*.ckpt` |\n",
    "| Training Metrics | `runs/<exp>/version_*/metrics.csv` |\n",
    "| Training Config | `runs/<exp>/training_config.json` |\n",
    "| Environment Snapshot | `runs/<exp>/environment_snapshot.txt` |\n",
    "| Figures (PNG + PDF) | `runs/<exp>/figures/` |\n",
    "| Figure Generation Script | `scripts/generate_<exp>_figures.py` |\n",
    "| Test Predictions | `predictions/<exp>_test/case_*.npz` |\n",
    "| Test Results | `predictions/<exp>_test/evaluation_results.json` |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created: YYYY-MM-DD*  \n",
    "*Last updated: YYYY-MM-DD*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
