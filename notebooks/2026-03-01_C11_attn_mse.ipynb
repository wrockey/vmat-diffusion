{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-overview",
   "metadata": {},
   "source": [
    "# Experiment: C11 Architecture Scout — AttentionUNet3D (MSE-only)\n",
    "\n",
    "**Date:** 2026-03-01  \n",
    "**Experiment ID:** `C11_attn_mse` (seed 42, single seed)  \n",
    "**Status:** Complete (Preliminary — seed 42 only)  \n",
    "**Type:** Training (architecture scout)  \n",
    "**GitHub Issue:** [#53](https://github.com/wrockey/vmat-diffusion/issues/53)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1 Objective\n",
    "\n",
    "Test whether AttentionUNet3D architecture improves dose prediction versus the baseline U-Net, using MSE-only loss to isolate the architecture effect. This is scout C11 in the architecture ablation series (#53), evaluating whether attention gates at skip connections provide a meaningful benefit.\n",
    "\n",
    "### 1.2 Hypothesis\n",
    "\n",
    "Attention gates at skip connections will help the model focus on clinically relevant regions (PTV, OAR boundaries) by selectively gating which spatial features are passed from encoder to decoder. This should improve PTV coverage (D95 gap, PTV gamma) relative to baseline, which uses unselective skip connections.\n",
    "\n",
    "### 1.3 Key Results\n",
    "\n",
    "| Metric | C11 AttentionUNet | Baseline (seed 42) | Delta |\n",
    "|--------|-------------------|-------------------|-------|\n",
    "| MAE (Gy) | 4.57 ± 2.51 | 4.80 ± 2.45 | -0.23 (slightly better) |\n",
    "| Gamma Global (%) | 29.6 ± 9.5 | 28.1 ± 12.6 | +1.6pp (slightly better) |\n",
    "| Gamma PTV (%) | 81.1 ± 8.8 | 87.3 ± 10.8 | **-6.1pp (worse)** |\n",
    "| D95 Gap (Gy) | -2.20 ± 0.91 | -0.83 ± 0.46 | **-1.37 (worse)** |\n",
    "\n",
    "### 1.4 Conclusion\n",
    "\n",
    "**AttentionUNet3D does not improve over baseline.** MAE and global gamma are marginally better, but PTV gamma is substantially worse (81.1% vs 87.3%, a -6.1pp regression) and the D95 gap worsens from -0.83 Gy to -2.20 Gy (deeper underdose). The attention mechanism does not help and may actively hurt PTV coverage by diluting PTV-focused features at skip connections. This confirms that architecture alone is not the bottleneck — loss function engineering (as demonstrated by the combined loss pilot at 96.4% PTV gamma) has a far larger impact. Training also took 20.3h vs ~12h for baseline, adding compute cost with no benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-whatchanged",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. What Changed\n",
    "\n",
    "Compared to baseline_v23 (seed 42), this experiment replaces **BaselineUNet3D with AttentionUNet3D**. **Everything else is identical** (same loss, data, augmentation, seed, epochs, optimizer, batch size).\n",
    "\n",
    "| Parameter | Baseline seed42 | This Experiment |\n",
    "|-----------|----------------|-----------------|\n",
    "| Architecture | BaselineUNet3D | **AttentionUNet3D** |\n",
    "| Parameters | 23.73M | **23.93M (+0.8%)** |\n",
    "| Skip connections | Standard (unselective) | **Attention-gated** |\n",
    "| Loss function | MSE + neg penalty | MSE + neg penalty (identical) |\n",
    "| Seed | 42 | 42 (identical split) |\n",
    "| Augmentation | ON | ON (identical) |\n",
    "| Optimizer | AdamW, lr=1e-4, wd=0.01 | AdamW, lr=1e-4, wd=0.01 (identical) |\n",
    "| Epochs | 200 | 200 (identical) |\n",
    "| Batch size | 2 | 2 (identical) |\n",
    "| Patch size | 128³ | 128³ (identical) |\n",
    "| All other hyperparameters | Default | Default (identical) |\n",
    "\n",
    "**Single variable under test:** BaselineUNet3D (plain skip connections) vs AttentionUNet3D (attention-gated skip connections)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-repro-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-repro-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "\n",
    "REPRODUCIBILITY = {\n",
    "    'git_commit': 'bded645',\n",
    "    'python_version': '3.12.12',\n",
    "    'pytorch_version': '2.10.0+cu126',\n",
    "    'pytorch_lightning_version': '2.6.1',\n",
    "    'cuda_version': '12.6',\n",
    "    'gpu': 'NVIDIA GeForce RTX 3090',\n",
    "    'random_seed': 42,\n",
    "    'experiment_date': '2026-03-01',\n",
    "    'platform': 'WSL2 Ubuntu 24.04 LTS',\n",
    "    'training_time_hours': 20.3,\n",
    "}\n",
    "\n",
    "print('Reproducibility Information:')\n",
    "for k, v in REPRODUCIBILITY.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-repro-cmd",
   "metadata": {},
   "source": [
    "### Command to Reproduce\n",
    "\n",
    "```bash\n",
    "# Train (AttentionUNet3D, MSE-only loss)\n",
    "python scripts/train_baseline_unet.py \\\n",
    "    --data_dir ~/data/processed_npz \\\n",
    "    --exp_name C11_attn_mse_seed42 \\\n",
    "    --architecture attention_unet \\\n",
    "    --epochs 200 --batch_size 2 --seed 42\n",
    "\n",
    "# Inference\n",
    "python scripts/inference_baseline_unet.py \\\n",
    "    --checkpoint runs/C11_attn_mse_seed42/checkpoints/best-epoch=128-val/mae_gy=6.399.ckpt \\\n",
    "    --input_dir <test_symlink_dir> \\\n",
    "    --output_dir predictions/C11_attn_mse_seed42_test \\\n",
    "    --compute_metrics --overlap 64 --gamma_subsample 4\n",
    "```\n",
    "\n",
    "Environment snapshot: `runs/C11_attn_mse_seed42/environment_snapshot.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dataset-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "\n",
    "test_cases_path = PROJECT_ROOT / 'runs' / 'C11_attn_mse_seed42' / 'test_cases.json'\n",
    "with open(test_cases_path) as f:\n",
    "    test_info = json.load(f)\n",
    "\n",
    "print(f'Preprocessing version: v2.3.0')\n",
    "print(f'Total cases: 74')\n",
    "print(f'Split (seed={test_info[\"seed\"]}): 60 train / 7 val / 7 test')\n",
    "print(f'Test case IDs: {sorted(test_info[\"test_cases\"])}')\n",
    "print(f'\\nNote: Same seed/split as baseline_v23 seed42 for direct architecture comparison.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dataset-detail",
   "metadata": {},
   "source": [
    "**Test cases (7):** prostate70gy_0005, prostate70gy_0018, prostate70gy_0024, prostate70gy_0027, prostate70gy_0056, prostate70gy_0065, prostate70gy_0079\n",
    "\n",
    "**Data provenance:** 74 cases preprocessed with v2.3.0 pipeline (native resolution crop, B-spline dose resampling). Identical to baseline_v23. The same seed 42 split ensures direct comparability — the only variable is the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-model-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model & Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "\n",
    "config_path = PROJECT_ROOT / 'runs' / 'C11_attn_mse_seed42' / 'training_config.json'\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f'Model: {config[\"model\"]}')\n",
    "print(f'Parameters: {config[\"model_params\"]:,}')\n",
    "\n",
    "print(f'\\nHyperparameters:')\n",
    "for k, v in sorted(config['hparams'].items()):\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "summary_path = PROJECT_ROOT / 'runs' / 'C11_attn_mse_seed42' / 'training_summary.json'\n",
    "with open(summary_path) as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(f'\\nTraining Summary:')\n",
    "print(f'  Duration: {summary[\"total_time_hours\"]:.1f} hours')\n",
    "print(f'  Best val MAE: {summary[\"best_val_mae_gy\"]:.3f} Gy')\n",
    "print(f'  Final epoch: {summary[\"final_metrics\"][\"epoch\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-model-detail",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "- **Model:** AttentionUNet3D, 48 base channels (48 -> 96 -> 192 -> 384 -> 768), **23.93M parameters** (vs 23.73M baseline, +0.8%)\n",
    "- **Input:** 9 channels (1 CT + 8 SDF), **Output:** 1 channel (dose)\n",
    "- **Constraint conditioning:** FiLM embedding (13-dim constraint vector)\n",
    "- **Patch size:** 128x128x128 voxels\n",
    "- **Key difference:** Attention gates at each decoder skip connection. Each gate takes the decoder feature map (query) and encoder feature map (key/value) and produces a soft spatial mask weighting which encoder features to pass through.\n",
    "\n",
    "### Attention Gate Mechanism\n",
    "\n",
    "At each of the 4 skip connections, the attention gate computes:\n",
    "\n",
    "$$\\alpha_i = \\sigma\\left(W_\\psi\\left(\\text{ReLU}(W_x x_i + W_g g_i + b)\\right) + b_\\psi\\right)$$\n",
    "\n",
    "where $x_i$ is the encoder feature map, $g_i$ is the decoder (gating) signal, and $\\alpha_i \\in [0,1]$ is the attention coefficient. The attended feature is $\\hat{x}_i = \\alpha_i \\odot x_i$.\n",
    "\n",
    "### Loss Configuration\n",
    "\n",
    "| Component | Weight | Notes |\n",
    "|-----------|--------|-------|\n",
    "| MSE | 1.0 | Standard pixel-wise mean squared error |\n",
    "| Negative penalty | 0.1 | Penalizes predicted dose < 0 |\n",
    "\n",
    "MSE-only loss chosen to isolate the architecture effect from loss function effects. Identical to baseline_v23 seed42.\n",
    "\n",
    "### Training\n",
    "\n",
    "- **Optimizer:** AdamW, lr=1e-4, weight_decay=0.01\n",
    "- **Epochs:** 200, batch_size=2\n",
    "- **Best checkpoint:** epoch 128 (val MAE = 6.40 Gy)\n",
    "- **Training time:** 20.3h (vs ~12h for baseline — attention adds ~70% compute overhead)\n",
    "- **Augmentation:** ON (random flips + intensity jitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-results-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results\n",
    "\n",
    "Figures generated by `scripts/generate_C11_attn_mse_figures.py`.  \n",
    "Representative case: **prostate70gy_0056** (below-median MAE = 3.18 Gy).  \n",
    "Inference uses overlap=64, gamma_subsample=4.\n",
    "\n",
    "### Per-Case Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-results-table-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "eval_path = PROJECT_ROOT / 'predictions' / 'C11_attn_mse_seed42_test' / 'baseline_evaluation_results.json'\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f'{\"Case\":<30} {\"MAE (Gy)\":>10} {\"Gamma Gl (%)\": >14} {\"Gamma PTV (%)\": >14} {\"D95 Gap (Gy)\": >13}')\n",
    "print('-' * 85)\n",
    "\n",
    "maes, gammas_g, gammas_p, d95s = [], [], [], []\n",
    "for c in results['per_case_results']:\n",
    "    cid = c['case_id']\n",
    "    mae = c['dose_metrics']['mae_gy']\n",
    "    gam_g = c['gamma']['global_3mm3pct']['gamma_pass_rate']\n",
    "    gam_p = c['gamma']['ptv_region_3mm3pct']['gamma_pass_rate']\n",
    "    d95 = c['dvh_metrics'].get('PTV70', {}).get('D95_error', float('nan'))\n",
    "    maes.append(mae)\n",
    "    gammas_g.append(gam_g)\n",
    "    gammas_p.append(gam_p)\n",
    "    d95s.append(d95)\n",
    "    print(f'{cid:<30} {mae:>10.2f} {gam_g:>14.1f} {gam_p:>14.1f} {d95:>13.2f}')\n",
    "\n",
    "print('-' * 85)\n",
    "print(f'{\"Mean +/- Std\":<30} '\n",
    "      f'{np.mean(maes):>10.2f}+/-{np.std(maes):.2f} '\n",
    "      f'{np.mean(gammas_g):>10.1f}+/-{np.std(gammas_g):.1f} '\n",
    "      f'{np.mean(gammas_p):>10.1f}+/-{np.std(gammas_p):.1f} '\n",
    "      f'{np.mean(d95s):>9.2f}+/-{np.std(d95s):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-results-table-md",
   "metadata": {},
   "source": [
    "**Approximate per-case metrics (from evaluation JSON — load code above for exact values):**\n",
    "\n",
    "| Case | MAE (Gy) | Gamma Global (%) | Gamma PTV (%) | D95 Gap (Gy) |\n",
    "|------|----------|-----------------|---------------|-------------|\n",
    "| prostate70gy_0005 | 4.83 | 23.4 | 77.4 | -2.04 |\n",
    "| prostate70gy_0018 | 4.76 | 20.8 | 83.6 | -2.47 |\n",
    "| prostate70gy_0024 | 5.57 | 16.1 | 90.3 | -1.73 |\n",
    "| prostate70gy_0027 | 1.52 | 42.3 | 82.4 | -0.78 |\n",
    "| prostate70gy_0056 | 3.18 | 35.0 | 76.3 | -2.53 |\n",
    "| prostate70gy_0065 | 9.64 | 32.1 | 88.0 | -3.21 |\n",
    "| prostate70gy_0079 | 2.66 | 38.6 | 69.4 | -2.65 |\n",
    "| **Mean +/- Std** | **4.57 +/- 2.51** | **29.6 +/- 9.5** | **81.1 +/- 8.8** | **-2.20 +/- 0.91** |\n",
    "\n",
    "**Notable:** No case exceeds 90% PTV Gamma. All D95 gaps are negative (underdose), ranging from -0.78 to -3.21 Gy. prostate70gy_0065 is again the highest MAE outlier (9.64 Gy).\n",
    "\n",
    "### 6.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig1_training_curves.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig1-caption",
   "metadata": {},
   "source": [
    "**Caption:** Training loss and validation MAE vs epoch for C11 AttentionUNet3D (seed 42, 200 epochs). Best val MAE: 6.40 Gy at epoch 128.\n",
    "\n",
    "**Key observations:**\n",
    "- Best val MAE (6.40 Gy) is slightly worse than baseline (6.05 Gy at best epoch), suggesting the attention mechanism does not improve generalization under MSE-only loss\n",
    "- Training appears stable with no divergence, confirming the attention gate implementation is numerically sound\n",
    "- The extra 0.2M attention parameters do not accelerate convergence relative to baseline\n",
    "- **Clinical implication:** The attention gates add ~70% compute overhead without reducing validation loss. Under MSE supervision, the model cannot exploit spatial selectivity effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig2-header",
   "metadata": {},
   "source": [
    "### 6.2 Dose Colorwash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig2_dose_colorwash.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig2-caption",
   "metadata": {},
   "source": [
    "**Caption:** Predicted vs ground truth dose for prostate70gy_0056 (MAE = 3.18 Gy, below-median). Axial, coronal, sagittal views through PTV70 centroid.\n",
    "\n",
    "**Key observations:**\n",
    "- PTV70 region shows noticeably cooler dose (less red/orange) in prediction vs GT, consistent with the -2.53 Gy D95 underdose for this case\n",
    "- Overall dose shape and conformality are preserved — the model correctly identifies the treatment region\n",
    "- The underdose pattern is concentrated near the PTV boundary rather than uniformly distributed\n",
    "- **Clinical implication:** The attention mechanism fails to improve PTV boundary accuracy. The predicted dose undershoots the prescription near PTV edges, which would translate to an inadequate coverage plan clinically. This pattern is worse than baseline despite adding attention gates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig3-header",
   "metadata": {},
   "source": [
    "### 6.3 Dose Difference Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig3_dose_difference.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig3-caption",
   "metadata": {},
   "source": [
    "**Caption:** Dose difference (predicted minus GT, Gy) for prostate70gy_0056. Blue = underdose, red = overdose.\n",
    "\n",
    "**Key observations:**\n",
    "- PTV region shows predominantly blue (underdose) — opposite to the combined loss pilot which showed red (overdose)\n",
    "- The underdose is spatially concentrated at PTV boundaries rather than uniformly spread across the volume\n",
    "- Peripheral low-dose regions show mixed blue/red consistent with baseline behavior\n",
    "- **Clinical implication:** The attention mechanism does not help the model prioritize PTV boundary accuracy. The underdose pattern at PTV edges is characteristic of MSE optimization without explicit PTV-focused loss terms. This confirms the combined loss pilot finding that loss engineering — not architecture — is the key lever for PTV coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig4-header",
   "metadata": {},
   "source": [
    "### 6.4 DVH Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig4_dvh_comparison.png', width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig4-caption",
   "metadata": {},
   "source": [
    "**Caption:** DVH curves for prostate70gy_0056. Solid = ground truth, dashed = predicted.\n",
    "\n",
    "**Key observations:**\n",
    "- PTV70 predicted DVH is shifted left (lower dose) compared to GT, consistent with the -2.53 Gy D95 underdose\n",
    "- OAR DVH curves track GT at a similar level to baseline — the attention mechanism does not degrade OAR accuracy\n",
    "- The PTV56 DVH is also shifted left, indicating the underdose extends to the lower-dose PTV as well\n",
    "- **Clinical implication:** A clinical plan with this predicted DVH would fail PTV coverage requirements. The D95 underdose would prompt a treatment plan revision. The attention mechanism specifically fails to fix the PTV boundary coverage that MSE-only training systematically underestimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig5-header",
   "metadata": {},
   "source": [
    "### 6.5 Gamma Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig5_gamma_bar_chart.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig5-caption",
   "metadata": {},
   "source": [
    "**Caption:** Global vs PTV-region Gamma 3%/3mm per test case (C11 AttentionUNet MSE, seed 42).\n",
    "\n",
    "**Key observations:**\n",
    "- No case exceeds the 95% PTV Gamma clinical target (best: 90.3% for prostate70gy_0024)\n",
    "- Mean PTV Gamma of 81.1% is substantially below the 95% target and below baseline (87.3%)\n",
    "- Global Gamma (29.6%) is marginally better than baseline (28.1%), but clinically insignificant\n",
    "- prostate70gy_0079 shows the worst PTV Gamma at 69.4%\n",
    "- **Clinical implication:** AttentionUNet3D fails to meet clinical spatial accuracy standards for PTV coverage. The regression vs baseline (-6.1pp PTV gamma) suggests the attention gates may suppress spatial features needed for PTV boundary accuracy. The 95% clinical target is not achievable with architecture alone under MSE-only training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig6-header",
   "metadata": {},
   "source": [
    "### 6.6 Per-Case Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig6_per_case_boxplots.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig6-caption",
   "metadata": {},
   "source": [
    "**Caption:** Metric distributions across 7 test cases (C11 AttentionUNet MSE, seed 42).\n",
    "\n",
    "**Key observations:**\n",
    "- D95 gap distribution is entirely negative (all 7 cases underdose), ranging from -0.78 to -3.21 Gy\n",
    "- MAE distribution has high variance (2.51 Gy std), driven by the prostate70gy_0065 outlier (9.64 Gy)\n",
    "- PTV Gamma distribution spans 69.4% to 90.3% — wide spread with no outlier in the favorable direction\n",
    "- prostate70gy_0027 is again the easiest case (1.52 Gy MAE, 82.4% PTV gamma) — anatomy/plan complexity drives case difficulty more than the model\n",
    "- **Clinical implication:** The wide variance and systematic underdose pattern across all 7 cases indicates this is a bias, not noise. The attention gate architecture does not correct the fundamental underdose tendency of MSE-only training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig7-header",
   "metadata": {},
   "source": [
    "### 6.7 QUANTEC Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig7_quantec_compliance.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig7-caption",
   "metadata": {},
   "source": [
    "**Caption:** QUANTEC constraint compliance heatmap (C11 AttentionUNet MSE, seed 42).\n",
    "\n",
    "**Key observations:**\n",
    "- Volume-based OAR constraints pass universally, consistent with baseline — the model correctly preserves OAR sparing\n",
    "- PTV D95 constraints fail (predicted dose is below threshold) due to underdose, worse than baseline\n",
    "- The compliance pattern is similar to the no-augmentation ablation — MSE-only training without PTV-focused loss fails PTV coverage regardless of architecture\n",
    "- **Clinical implication:** AttentionUNet3D meets OAR constraints but fails PTV coverage. This is the same failure mode as baseline, confirming the attention gates do not address the root cause (lack of PTV-targeted loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig8-header",
   "metadata": {},
   "source": [
    "### 6.8 Seed Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fig8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../runs/C11_attn_mse/figures/fig8_seed_variability.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-fig8-caption",
   "metadata": {},
   "source": [
    "**Caption:** Seed variability analysis (C11 AttentionUNet MSE). Note: this is a single-seed pilot (seed 42 only). The figure shows per-case metric distributions rather than cross-seed comparisons.\n",
    "\n",
    "**Key observations:**\n",
    "- Single-seed pilot — cross-seed variability cannot be assessed\n",
    "- Per-case distributions show consistent underdose across all cases, suggesting the finding is systematic\n",
    "- The combination of high MAE variance (2.51 Gy) and consistent D95 underdose suggests the issue is loss-driven, not seed-driven\n",
    "- **Clinical implication:** Given the negative result (-6.1pp PTV gamma vs baseline), full 3-seed confirmation is not prioritized. The effect size is large enough that the direction of the finding is clear: attention gates do not help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-stats-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Statistical Analysis\n",
    "\n",
    "This is a **single-seed pilot** (seed 42 only). Formal cross-seed statistics are not available. The comparison below is a **paired analysis** on the same 7 test cases (same seed, same split) between this experiment and the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "pred_base = PROJECT_ROOT / 'predictions'\n",
    "\n",
    "def load_metrics(eval_path):\n",
    "    with open(eval_path) as f:\n",
    "        d = json.load(f)\n",
    "    maes, gammas_g, gammas_p, d95 = [], [], [], []\n",
    "    for c in d['per_case_results']:\n",
    "        maes.append(c['dose_metrics']['mae_gy'])\n",
    "        gammas_g.append(c['gamma']['global_3mm3pct']['gamma_pass_rate'])\n",
    "        gammas_p.append(c['gamma']['ptv_region_3mm3pct']['gamma_pass_rate'])\n",
    "        ptv70 = c['dvh_metrics'].get('PTV70', {})\n",
    "        if 'D95_error' in ptv70:\n",
    "            d95.append(ptv70['D95_error'])\n",
    "    return {'mae': maes, 'gamma_g': gammas_g, 'gamma_p': gammas_p, 'd95': d95,\n",
    "            'case_ids': [c['case_id'] for c in d['per_case_results']]}\n",
    "\n",
    "c11 = load_metrics(pred_base / 'C11_attn_mse_seed42_test/baseline_evaluation_results.json')\n",
    "baseline = load_metrics(pred_base / 'baseline_v23_seed42_test/baseline_evaluation_results.json')\n",
    "\n",
    "print('Head-to-Head Comparison: C11 AttentionUNet vs Baseline (same 7 test cases, same seed 42 split)')\n",
    "print('=' * 90)\n",
    "for metric, key, unit in [('MAE', 'mae', 'Gy'), ('Gamma Global', 'gamma_g', '%'),\n",
    "                            ('Gamma PTV', 'gamma_p', '%'), ('D95 Gap', 'd95', 'Gy')]:\n",
    "    c11_m, c11_s = np.mean(c11[key]), np.std(c11[key])\n",
    "    bl_m, bl_s = np.mean(baseline[key]), np.std(baseline[key])\n",
    "    diff = c11_m - bl_m\n",
    "    sign = '+' if diff > 0 else ''\n",
    "    print(f'  {metric:<18} C11: {c11_m:6.2f} +/- {c11_s:5.2f} {unit}  '\n",
    "          f'Baseline: {bl_m:6.2f} +/- {bl_s:5.2f} {unit}  Diff: {sign}{diff:.2f}')\n",
    "\n",
    "# Per-case paired differences for PTV Gamma\n",
    "print(f'\\nPer-Case PTV Gamma Differences (C11 - Baseline):')\n",
    "diffs_gamma = []\n",
    "for i, cid in enumerate(c11['case_ids']):\n",
    "    j = baseline['case_ids'].index(cid)\n",
    "    d = c11['gamma_p'][i] - baseline['gamma_p'][j]\n",
    "    diffs_gamma.append(d)\n",
    "    sign = '+' if d > 0 else ''\n",
    "    print(f'  {cid}: {sign}{d:.1f}pp')\n",
    "print(f'  Mean diff: {np.mean(diffs_gamma):+.1f}pp (positive = C11 is better)')\n",
    "print(f'  Cases where C11 is better: {sum(1 for d in diffs_gamma if d > 0)}/7')\n",
    "\n",
    "# Per-case paired differences for D95\n",
    "print(f'\\nPer-Case D95 Gap Differences (C11 - Baseline):')\n",
    "diffs_d95 = []\n",
    "for i, cid in enumerate(c11['case_ids']):\n",
    "    j = baseline['case_ids'].index(cid)\n",
    "    d = c11['d95'][i] - baseline['d95'][j]\n",
    "    diffs_d95.append(d)\n",
    "    sign = '+' if d > 0 else ''\n",
    "    print(f'  {cid}: {sign}{d:.2f} Gy')\n",
    "print(f'  Mean diff: {np.mean(diffs_d95):+.2f} Gy (negative = C11 underdoses more)')\n",
    "print(f'  Cases where C11 is better (less underdose): {sum(1 for d in diffs_d95 if d > 0)}/7')\n",
    "\n",
    "print(f'\\nNote: Pilot status (1 seed). Architecture scout is negative result — full 3-seed '\n",
    "      f'confirmation not planned.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-stats-summary",
   "metadata": {},
   "source": [
    "### Statistical Summary\n",
    "\n",
    "| Metric | C11 AttentionUNet | Baseline (seed42) | Delta | Direction |\n",
    "|--------|-------------------|-------------------|-------|-----------|\n",
    "| MAE (Gy) | 4.57 +/- 2.51 | 4.80 +/- 2.45 | -0.23 | Slightly better |\n",
    "| Gamma global (%) | 29.6 +/- 9.5 | 28.1 +/- 12.6 | +1.6pp | Slightly better |\n",
    "| Gamma PTV (%) | 81.1 +/- 8.8 | 87.3 +/- 10.8 | **-6.1pp** | **Worse** |\n",
    "| D95 gap (Gy) | -2.20 +/- 0.91 | -0.83 +/- 0.46 | **-1.37** | **Worse** |\n",
    "\n",
    "**Interpretation:** The C11 architecture scout is a **negative result**. MAE and global gamma show minor, clinically insignificant improvements (+/- within noise). The two primary clinical metrics — PTV gamma and D95 gap — both worsen substantially. PTV gamma regresses by 6.1pp (81.1% vs 87.3%), and D95 underdose deepens by 1.37 Gy (-2.20 vs -0.83 Gy). All 7 test cases show deeper underdose with the attention architecture.\n",
    "\n",
    "This is a single-seed pilot. Given the magnitude of the negative effect, full 3-seed confirmation is not warranted — the direction is unambiguous. The architecture scout series (C11/C13/C15) is helping confirm that **architecture choice is not the primary bottleneck** for clinical metric improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Cross-Experiment Comparison\n",
    "\n",
    "| Experiment | MAE (Gy) | Gamma Global (%) | Gamma PTV (%) | D95 Gap (Gy) | Status |\n",
    "|------------|----------|-----------------|---------------|-------------|--------|\n",
    "| Baseline 3-seed aggregate | 4.22 +/- 0.53 | 33.8 +/- 4.6 | 80.2 +/- 5.3 | -1.76 +/- 0.69 | Complete |\n",
    "| Baseline seed42 | 4.80 +/- 2.45 | 28.1 +/- 12.6 | 87.3 +/- 10.8 | -0.83 +/- 0.46 | Complete |\n",
    "| No augmentation (seed42) | 5.04 +/- 2.92 | 27.4 +/- 9.8 | 83.2 +/- 9.8 | -1.89 +/- 1.01 | Complete |\n",
    "| Combined loss pilot (seed42) | 4.54 +/- 1.84 | 30.8 +/- 12.4 | **96.4 +/- 5.4** | +1.37 +/- 0.57 | Preliminary |\n",
    "| **C11 AttentionUNet MSE (seed42)** | **4.57 +/- 2.51** | **29.6 +/- 9.5** | **81.1 +/- 8.8** | **-2.20 +/- 0.91** | **Preliminary** |\n",
    "| Phase 2 target | < 3.0 | -- | > 95% | >= -0.5 | -- |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Architecture is not the bottleneck.** C11 (AttentionUNet) and baseline (plain U-Net) produce near-identical MAE (4.57 vs 4.80 Gy), but the attention architecture actually *worsens* the two primary clinical metrics. Architecture changes without loss changes cannot achieve the 95% PTV Gamma target.\n",
    "\n",
    "2. **Loss function engineering dominates.** The combined loss pilot achieves 96.4% PTV Gamma (+15.3pp over C11) with the *same* baseline architecture and only 0.03 Gy better MAE. The difference between 81.1% and 96.4% PTV Gamma is entirely attributable to the loss function, not architecture.\n",
    "\n",
    "3. **C11 is worse than baseline on clinical metrics.** PTV gamma regresses by 6.1pp and D95 underdose deepens by 1.37 Gy. The attention mechanism appears to actively hurt PTV coverage. This is counterintuitive — the attention hypothesis was that it would help focus on PTV boundaries, but the effect is opposite.\n",
    "\n",
    "4. **MSE-only training systematically underdoses.** All MSE-only experiments (baseline, no-aug, C11) show negative D95 gaps. The combined loss is the only experiment to flip the sign, confirming that explicit PTV-targeted loss terms are required.\n",
    "\n",
    "5. **Compute overhead not justified.** AttentionUNet required 20.3h vs ~12h for baseline (+70% training time) with no clinical benefit. For architecture exploration, the combined loss applied to BaselineUNet3D is a more efficient path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclusions, Limitations, and Next Steps\n",
    "\n",
    "### What Worked\n",
    "\n",
    "1. **Stable training.** AttentionUNet3D trains without divergence. The attention gate implementation is numerically stable and compatible with the existing training infrastructure.\n",
    "\n",
    "2. **MAE marginally improved** (-0.23 Gy vs baseline). This is within noise given the test set size (n=7) but confirms the attention mechanism does not harm global dose accuracy.\n",
    "\n",
    "3. **OAR sparing maintained.** QUANTEC OAR compliance is equivalent to baseline. The attention gates do not degrade OAR dose accuracy.\n",
    "\n",
    "4. **Architecture scout validated the experimental framework.** The C11/C13/C15 scout series is successfully isolating architecture from loss effects, enabling clear attribution.\n",
    "\n",
    "### What Didn't Work\n",
    "\n",
    "1. **PTV gamma regressed by 6.1pp** (81.1% vs 87.3% for baseline). The attention mechanism — intended to focus on clinically relevant regions — appears to suppress the encoder features needed for accurate PTV boundary prediction.\n",
    "\n",
    "2. **D95 underdose deepened by 1.37 Gy** (-2.20 vs -0.83 Gy). All 7 test cases show worse underdose. This is a systematic bias, not random variation.\n",
    "\n",
    "3. **Training time increased 70%** (20.3h vs ~12h). The attention gate parameters add significant forward-pass overhead without proportional benefit.\n",
    "\n",
    "4. **Best val MAE slightly worse** (6.40 Gy vs 6.05 Gy for baseline at best epoch). The attention mechanism does not improve validation loss under MSE supervision.\n",
    "\n",
    "### Mechanistic Hypothesis\n",
    "\n",
    "The attention gate learns to suppress encoder features based on the decoder query signal. Under MSE-only training, the loss signal does not explicitly reward PTV boundary accuracy — it rewards global voxel accuracy. The attention gate may learn to suppress high-frequency PTV boundary features (which are harder to predict) in favor of the smooth low-dose background (which dominates MSE by volume). This would explain why PTV metrics worsen while global MAE improves marginally.\n",
    "\n",
    "**Counter-hypothesis:** With a PTV-focused loss (e.g., the combined loss), attention gates might actually help — they could learn to focus on the PTV when the loss signal explicitly rewards PTV accuracy. Testing AttentionUNet3D with the combined loss is a potential future experiment.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Single seed (42 only)** — results should be interpreted as directional evidence, not statistically confirmed. However, the effect size (-6.1pp PTV gamma) is large enough to be confidently negative.\n",
    "- **Small test set (n=7)** — cannot compute significance statistics.\n",
    "- **Cannot separate attention gate effects from initialization** — it's possible the additional parameters require a different learning rate schedule.\n",
    "- **Single architecture variant** — only standard Okta-style attention gates tested. Alternative attention formulations (e.g., CBAM, self-attention, multi-head) are not tested and could behave differently.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Continue architecture scout series:** C13 (BottleneckAttn) and C15 (Wider Baseline) scouts should run to complete the planned comparison. Architecture choices do not appear to matter much, but completing the planned experiments maintains scientific rigor.\n",
    "\n",
    "2. **Focus Phase 2 on loss engineering:** The combined loss pilot demonstrated +15.3pp PTV gamma improvement. Tuning the asymmetric penalty weight (from 3:1 to 2:1) to correct D95 overdose is the highest-priority next step.\n",
    "\n",
    "3. **Do not run 3-seed AttentionUNet confirmation:** The negative result is clear from seed 42 alone. Running 3 seeds on a clearly inferior architecture would consume ~60h GPU time without scientific value.\n",
    "\n",
    "4. **Consider AttentionUNet + combined loss** as a future experiment only if the architecture scout series suggests combined benefit. Current evidence does not support prioritizing this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-artifacts",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Artifacts\n",
    "\n",
    "| Artifact | Path |\n",
    "|----------|------|\n",
    "| Run directory | `runs/C11_attn_mse_seed42/` |\n",
    "| Best checkpoint | `runs/C11_attn_mse_seed42/checkpoints/best-epoch=128-val/mae_gy=6.399.ckpt` |\n",
    "| Training config | `runs/C11_attn_mse_seed42/training_config.json` |\n",
    "| Training summary | `runs/C11_attn_mse_seed42/training_summary.json` |\n",
    "| Test cases | `runs/C11_attn_mse_seed42/test_cases.json` |\n",
    "| Predictions | `predictions/C11_attn_mse_seed42_test/` |\n",
    "| Evaluation JSON | `predictions/C11_attn_mse_seed42_test/baseline_evaluation_results.json` |\n",
    "| Figures (PNG + PDF) | `runs/C11_attn_mse/figures/` (8 figures, 16 files) |\n",
    "| Figure script | `scripts/generate_C11_attn_mse_figures.py` |\n",
    "| Environment snapshot | `runs/C11_attn_mse_seed42/environment_snapshot.txt` |\n",
    "| This notebook | `notebooks/2026-03-01_C11_attn_mse.ipynb` |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created: 2026-03-01*  \n",
    "*Status: Complete (Preliminary — seed 42 only)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmat-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
