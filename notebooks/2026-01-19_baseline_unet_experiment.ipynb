{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Baseline U-Net for VMAT Dose Prediction\n",
    "\n",
    "**Date:** 2026-01-19  \n",
    "**Experiment ID:** `baseline_unet_run1`  \n",
    "**Status:** Complete  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1 Objective\n",
    "Train a baseline 3D U-Net model for direct dose prediction from CT and structure data. This serves as:\n",
    "1. Validation that the preprocessing pipeline produces usable training data\n",
    "2. A baseline for comparison against diffusion-based models (DDPM)\n",
    "3. Proof of concept for the dose prediction task\n",
    "\n",
    "### 1.2 Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Best Validation MAE** | **3.73 Gy** |\n",
    "| Training Duration | 2.55 hours |\n",
    "| Epochs (early stopped) | 62/200 |\n",
    "| Best Epoch | 12 |\n",
    "\n",
    "### 1.3 Conclusion\n",
    "The baseline U-Net achieves reasonable dose prediction accuracy (3.73 Gy MAE), demonstrating that:\n",
    "- The preprocessed data is suitable for training\n",
    "- The model architecture can learn the dose distribution\n",
    "- Early stopping triggered at epoch 62, suggesting convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Reproducibility Information\n",
    "\n",
    "### 2.1 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment snapshot at time of experiment\n",
    "REPRODUCIBILITY_INFO = {\n",
    "    'git_commit': '0e2fedc74fd75899cf1ead5488b63a96e0bbf455',\n",
    "    'git_message': 'Fix SDF computation bug: cast uint8 mask to bool before bitwise NOT',\n",
    "    'python_version': '3.12.12',\n",
    "    'pytorch_version': '2.4.1',\n",
    "    'cuda_version': '12.4',\n",
    "    'gpu': 'NVIDIA GeForce RTX 3090',\n",
    "    'gpu_memory_gb': 24,\n",
    "    'random_seed': 42,\n",
    "    'experiment_date': '2026-01-19',\n",
    "    'preprocessing_version': '2.2.0',\n",
    "}\n",
    "\n",
    "for k, v in REPRODUCIBILITY_INFO.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Command to Reproduce\n",
    "\n",
    "```bash\n",
    "# Ensure git is at the correct commit\n",
    "git checkout 0e2fedc74fd75899cf1ead5488b63a96e0bbf455\n",
    "\n",
    "# Activate environment\n",
    "conda activate vmat-diffusion\n",
    "\n",
    "# Run training\n",
    "python scripts/train_baseline_unet.py \\\n",
    "    --data_dir /mnt/i/processed_npz \\\n",
    "    --epochs 200 \\\n",
    "    --exp_name baseline_unet_run1 \\\n",
    "    --seed 42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dataset\n",
    "\n",
    "### 3.1 Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_INFO = {\n",
    "    'total_cases': 23,\n",
    "    'skipped_cases': 1,  # case_0013 - missing PTV56 (non-SIB)\n",
    "    'total_size_gb': 4.6,\n",
    "    'preprocessing_script': 'preprocess_dicom_rt_v2.2.py',\n",
    "    'volume_shape': (512, 512, 256),\n",
    "    'voxel_spacing_mm': (1.0, 1.0, 2.0),\n",
    "    'structures': ['PTV70', 'PTV56', 'Prostate', 'Rectum', 'Bladder', 'Femur_L', 'Femur_R', 'Bowel'],\n",
    "    'input_channels': 9,  # 1 CT + 8 SDF\n",
    "    'constraint_dim': 13,\n",
    "}\n",
    "\n",
    "print('Dataset Summary:')\n",
    "for k, v in DATASET_INFO.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test case list (held out during training)\n",
    "test_cases_path = Path('../runs/baseline_unet_run1/test_cases.json')\n",
    "if test_cases_path.exists():\n",
    "    with open(test_cases_path) as f:\n",
    "        test_cases = json.load(f)\n",
    "    print('Test cases (held out):')\n",
    "    for case in test_cases:\n",
    "        print(f'  - {case}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Split\n",
    "\n",
    "| Split | Cases | Percentage |\n",
    "|-------|-------|------------|\n",
    "| Train | 19 | 83% |\n",
    "| Validation | 2 | 9% |\n",
    "| Test | 2 | 9% |\n",
    "\n",
    "**Note:** Random split with seed=42 for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Architecture\n",
    "\n",
    "### 4.1 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'architecture': 'BaselineUNet3D',\n",
    "    'type': 'Direct Regression (not diffusion)',\n",
    "    'parameters': 23_732_801,\n",
    "    'parameters_human': '23.7M',\n",
    "    'input_channels': 9,  # 1 CT + 8 SDF channels\n",
    "    'output_channels': 1,  # Dose\n",
    "    'base_channels': 48,\n",
    "    'encoder_channels': [48, 96, 192, 384],\n",
    "    'bottleneck_channels': 768,\n",
    "    'constraint_conditioning': 'FiLM (Feature-wise Linear Modulation)',\n",
    "    'constraint_dim': 13,\n",
    "    'normalization': 'GroupNorm',\n",
    "    'activation': 'SiLU',\n",
    "    'upsampling': 'Trilinear + Conv',\n",
    "}\n",
    "\n",
    "print('Model Configuration:')\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Architecture Diagram\n",
    "\n",
    "```\n",
    "Input: CT (1) + SDF (8) = 9 channels, 128³ patches\n",
    "       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Encoder                                                │\n",
    "│  Conv3D(9→48) → Conv3D(48→96) → Conv3D(96→192) → (384) │\n",
    "│  + MaxPool3D at each level                              │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Bottleneck (384→768→384)                               │\n",
    "│  + FiLM conditioning from constraints (13-dim)          │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Decoder with Skip Connections                          │\n",
    "│  Upsample + Concat + Conv3D at each level               │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "       ↓\n",
    "Output: Dose (1 channel), 128³\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    'max_epochs': 200,\n",
    "    'actual_epochs': 62,\n",
    "    'early_stopping': True,\n",
    "    'early_stopping_patience': 50,\n",
    "    'batch_size': 2,\n",
    "    'patch_size': 128,\n",
    "    'patches_per_volume': 4,\n",
    "    'samples_per_epoch': 76,  # 19 train cases × 4 patches\n",
    "    'optimizer': 'AdamW',\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_scheduler': 'CosineAnnealingLR',\n",
    "    'loss_function': 'MSE + Gradient Loss',\n",
    "    'precision': '16-mixed (AMP)',\n",
    "    'gradient_clip': 1.0,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print('Training Configuration:')\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Results\n",
    "\n",
    "### 6.1 Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set publication-quality defaults\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.figsize': (8, 6),\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "# Load metrics\n",
    "metrics_path = Path('../runs/baseline_unet_run1/version_3/metrics.csv')\n",
    "df = pd.read_csv(metrics_path)\n",
    "\n",
    "# Extract epoch-level metrics\n",
    "epoch_metrics = df[df['val/mae_gy'].notna()][['epoch', 'val/loss', 'val/mae_gy']].copy()\n",
    "epoch_metrics['epoch'] = epoch_metrics['epoch'].astype(int)\n",
    "\n",
    "# Also get training loss per epoch\n",
    "train_loss = df[df['train/loss_epoch'].notna()][['epoch', 'train/loss_epoch']].copy()\n",
    "train_loss['epoch'] = train_loss['epoch'].astype(int)\n",
    "\n",
    "# Merge\n",
    "metrics = epoch_metrics.merge(train_loss, on='epoch', how='left')\n",
    "metrics.columns = ['epoch', 'val_loss', 'val_mae_gy', 'train_loss']\n",
    "\n",
    "print(f'Loaded {len(metrics)} epochs of metrics')\n",
    "metrics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(metrics['epoch'], metrics['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(metrics['epoch'], metrics['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, metrics['epoch'].max())\n",
    "\n",
    "# Plot 2: Validation MAE\n",
    "ax2 = axes[1]\n",
    "ax2.plot(metrics['epoch'], metrics['val_mae_gy'], 'g-', linewidth=2)\n",
    "best_epoch = metrics.loc[metrics['val_mae_gy'].idxmin(), 'epoch']\n",
    "best_mae = metrics['val_mae_gy'].min()\n",
    "ax2.axhline(y=best_mae, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_mae:.2f} Gy')\n",
    "ax2.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7)\n",
    "ax2.scatter([best_epoch], [best_mae], color='r', s=100, zorder=5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation MAE (Gy)')\n",
    "ax2.set_title('Validation Mean Absolute Error')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, metrics['epoch'].max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../runs/baseline_unet_run1/training_curves.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nBest validation MAE: {best_mae:.3f} Gy at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training summary\n",
    "with open('../runs/baseline_unet_run1/training_summary.json') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "RESULTS = {\n",
    "    'training_time_hours': round(summary['total_time_hours'], 2),\n",
    "    'total_epochs': summary['final_metrics']['epoch'],\n",
    "    'best_val_mae_gy': round(summary['best_val_mae_gy'], 3),\n",
    "    'final_val_mae_gy': round(summary['final_metrics']['val_mae_gy'], 3),\n",
    "    'final_train_loss': round(summary['final_metrics']['train_loss'], 6),\n",
    "    'final_val_loss': round(summary['final_metrics']['val_loss'], 6),\n",
    "    'early_stopped': summary['final_metrics']['epoch'] < 199,\n",
    "    'best_checkpoint': 'best-epoch=012-val/mae_gy=3.735.ckpt',\n",
    "}\n",
    "\n",
    "print('=' * 60)\n",
    "print('FINAL RESULTS')\n",
    "print('=' * 60)\n",
    "for k, v in RESULTS.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analysis\n",
    "\n",
    "### 7.1 Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence\n",
    "print('Convergence Analysis:')\n",
    "print(f'  - Training stopped at epoch {RESULTS[\"total_epochs\"]} (max: 200)')\n",
    "print(f'  - Best MAE achieved at epoch {best_epoch}')\n",
    "print(f'  - Epochs after best: {RESULTS[\"total_epochs\"] - best_epoch}')\n",
    "print(f'  - Early stopping patience: 50 epochs')\n",
    "print(f'  - Final MAE ({RESULTS[\"final_val_mae_gy\"]:.2f} Gy) vs Best ({RESULTS[\"best_val_mae_gy\"]:.2f} Gy)')\n",
    "print(f'  - Degradation: {RESULTS[\"final_val_mae_gy\"] - RESULTS[\"best_val_mae_gy\"]:.2f} Gy')\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_gap = metrics['val_loss'].iloc[-1] - metrics['train_loss'].iloc[-1]\n",
    "print(f'\\nOverfitting check:')\n",
    "print(f'  - Final train loss: {metrics[\"train_loss\"].iloc[-1]:.6f}')\n",
    "print(f'  - Final val loss: {metrics[\"val_loss\"].iloc[-1]:.6f}')\n",
    "print(f'  - Gap: {train_val_gap:.6f}')\n",
    "print(f'  - Assessment: {\"Minimal overfitting\" if train_val_gap < 0.01 else \"Some overfitting observed\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Clinical Interpretation\n",
    "\n",
    "**MAE of 3.73 Gy in context:**\n",
    "- Prescription dose: 70 Gy to PTV70\n",
    "- 3.73 Gy ≈ 5.3% of prescription dose\n",
    "- This is a reasonable baseline, but clinical acceptability typically requires:\n",
    "  - DVH-based metrics\n",
    "  - Gamma analysis (3%/3mm)\n",
    "  - Structure-specific dose accuracy\n",
    "\n",
    "**Next steps for clinical validation:**\n",
    "1. Run inference on held-out test cases\n",
    "2. Compute DVH comparisons\n",
    "3. Gamma analysis\n",
    "4. Visualize dose distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Artifacts and Outputs\n",
    "\n",
    "### 8.1 Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_dir = Path('../runs/baseline_unet_run1')\n",
    "\n",
    "print('Saved artifacts:')\n",
    "print(f'\\nDirectory: {run_dir.absolute()}')\n",
    "print('\\nFiles:')\n",
    "for f in sorted(run_dir.rglob('*')):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size\n",
    "        if size > 1e6:\n",
    "            size_str = f'{size/1e6:.1f} MB'\n",
    "        elif size > 1e3:\n",
    "            size_str = f'{size/1e3:.1f} KB'\n",
    "        else:\n",
    "            size_str = f'{size} B'\n",
    "        rel_path = f.relative_to(run_dir)\n",
    "        print(f'  {rel_path}: {size_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 How to Load Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to load the trained model\n",
    "LOAD_MODEL_CODE = '''\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to best checkpoint\n",
    "ckpt_path = Path('runs/baseline_unet_run1/checkpoints/best-epoch=012-val/mae_gy=3.735.ckpt')\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "\n",
    "# If using the LightningModule directly:\n",
    "# model = BaselineUNetModule.load_from_checkpoint(ckpt_path)\n",
    "# model.eval()\n",
    "\n",
    "# Or extract just the state dict:\n",
    "# state_dict = checkpoint['state_dict']\n",
    "'''\n",
    "print(LOAD_MODEL_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "### Immediate:\n",
    "1. [ ] Run inference on test cases (case_0009, case_0022)\n",
    "2. [ ] Generate DVH comparisons\n",
    "3. [ ] Compute gamma pass rates\n",
    "4. [ ] Visualize predicted vs ground truth doses\n",
    "\n",
    "### Future experiments:\n",
    "1. [ ] Train DDPM model for comparison\n",
    "2. [ ] Ablation study: with/without SDF features\n",
    "3. [ ] Ablation study: with/without constraint conditioning\n",
    "4. [ ] Increase dataset size (process case_0013 with --relax_filter)\n",
    "5. [ ] Hyperparameter tuning (learning rate, batch size)\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Notes and Observations\n",
    "\n",
    "### Issues encountered:\n",
    "1. **Gamma computation failed** - pymedphys gamma function had module import issue (non-critical)\n",
    "2. **Deterministic warnings** - trilinear upsampling backward pass not deterministic on CUDA\n",
    "\n",
    "### Observations:\n",
    "1. Best MAE achieved early (epoch 12), suggesting model learns quickly\n",
    "2. Validation MAE fluctuates significantly (3.7-9.4 Gy range)\n",
    "3. Small validation set (2 cases) may contribute to variance\n",
    "4. Training loss continues to decrease after validation plateaus\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook generated: 2026-01-19*  \n",
    "*Last updated: 2026-01-19*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
